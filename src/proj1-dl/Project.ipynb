{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import dlc_bci as bci\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = bci.load(root = './data_bci')\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "X = train_input.numpy()\n",
    "y = train_target.numpy()\n",
    "kfolds = model_selection.KFold(n_splits=10, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is done!\n"
     ]
    }
   ],
   "source": [
    "# put this inside the train to avoid data snooping\n",
    "mu, std = train_input.mean(0), train_input.std(0)\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "print(\"Normalization is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input  = Variable(test_input)\n",
    "test_target = Variable(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_full(param, train_input, train_target, kfolds, nb_epochs, lambdda = 0.01, lr = 0.001):\n",
    "    \n",
    "    acc_train_kfold = []\n",
    "    loss_train_kfold = []\n",
    "    acc_val_kfold = []\n",
    "    loss_val_kfold = []\n",
    "    \n",
    "    for train_index, val_index in kfolds.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_train = Variable(torch.from_numpy(X_train))\n",
    "        X_val = Variable(torch.from_numpy(X_val))\n",
    "        y_train = Variable(torch.from_numpy(y_train))\n",
    "        y_val = Variable(torch.from_numpy(y_val)) \n",
    "        \n",
    "        if param['type']=='fc': model = FC_net(param['layers'])\n",
    "        elif param['type']=='conv': model = model_conv = Conv_net(param['layers'], param['layers_conv'], \\\n",
    "                                                    param['kernel_size'], param['pooling_kernel_size'], param['p_list'])\n",
    "        \n",
    "        loss_train, loss_val, acc_train, acc_val = train_model(model, X_train, y_train, X_val, y_val, kfolds, nb_epochs, lambdda, lr)\n",
    "        acc_train_kfold.append(acc_train)\n",
    "        loss_train_kfold.append(loss_train)\n",
    "        acc_val_kfold.append(acc_val)\n",
    "        loss_val_kfold.append(loss_val)\n",
    "        \n",
    "    acc_train_kfold = np.mean(np.array(acc_train_kfold), axis=0)\n",
    "    acc_val_kfold = np.mean(np.array(acc_val_kfold), axis=0)\n",
    "\n",
    "    loss_train_kfold = np.mean(np.array(loss_train_kfold), axis=0)\n",
    "    loss_val_kfold = np.mean(np.array(loss_val_kfold), axis=0)\n",
    "    \n",
    "    print('\\n\\n---- Epochs Done -----\\n')\n",
    "    print('Loss: train ~ {} Acc train ~ {} \\n   Loss: val ~ {} / Acc val ~ {}\\n'\n",
    "         .format(round(loss_train_kfold[-1], 3), \n",
    "                 round(acc_train_kfold[-1], 3), \n",
    "                 round(loss_val_kfold[-1], 3), \n",
    "                 round(acc_val_kfold[-1], 3)))\n",
    "    return loss_train_kfold, loss_val_kfold, acc_train_kfold, acc_val_kfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, kfolds, nb_epochs, lambdda = 0.01, lr = 0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "        \n",
    "    for e in range(0, nb_epochs):\n",
    "         \n",
    "        model.train(True)\n",
    "        for b in list(range(0, X_train.size(0), mini_batch_size)):\n",
    "            if b + mini_batch_size <= X_train.size(0):\n",
    "                output = model(X_train.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, y_train.narrow(0, b, mini_batch_size))\n",
    "            else:\n",
    "                output = model(X_train.narrow(0, b, X_train.size(0) - b))\n",
    "                loss = criterion(output, y_train.narrow(0, b, X_train.size(0) - b))\n",
    "\n",
    "            for p in model.parameters():\n",
    "                loss += lambdda*p.pow(2).sum()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            model.train(False)\n",
    "            output_train = model(X_train)\n",
    "            output_val = model(X_val)\n",
    "            \n",
    "        acc_val.append(1-compute_nb_errors(model, X_val, y_val, mini_batch_size=mini_batch_size)/X_val.size(0))\n",
    "        acc_train.append(1-compute_nb_errors(model, X_train, y_train, mini_batch_size=mini_batch_size)/X_train.size(0))\n",
    "        loss_train.append(criterion(output_train, y_train).data[0])\n",
    "        loss_val.append(criterion(output_val, y_val).data[0])\n",
    "        \n",
    "#         if (e % 100 == 0):\n",
    "#                 print('Epoch {}: \\n   CVLoss: train ~ {} CVAcc train ~ {} \\n   CVLoss: val ~ {} / CVAcc val ~ {}'\n",
    "#                   .format(e, \n",
    "#                           round(loss_train_epoch[-1], 3),\n",
    "#                           round(acc_train_epoch[-1], 3), \n",
    "#                           round(loss_val_epoch[-1], 3),\n",
    "#                           round(acc_val_epoch[-1], 3)))\n",
    "        \n",
    "#     print('\\n\\n---- Epochs Done -----\\n')\n",
    "#     print('CVLoss: train ~ {} CVAcc train ~ {} \\n   CVLoss: val ~ {} / CVAcc val ~ {}'\n",
    "#          .format(round(loss_train_epoch[-1], 3), \n",
    "#                  round(acc_train_epoch[-1], 3), \n",
    "#                  round(loss_val_epoch[-1], 3), \n",
    "#                  round(acc_val_epoch[-1], 3)))\n",
    "\n",
    "    return loss_train, loss_val, acc_train, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        if b + mini_batch_size <= data_input.size(0):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = torch.max(output.data, 1)\n",
    "            for k in range(0, mini_batch_size):\n",
    "                if data_target.data[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "        else:       \n",
    "            output = model(data_input.narrow(0, b, data_input.size(0) - b))\n",
    "            _, predicted_classes = torch.max(output.data, 1)\n",
    "            for k in range(0, data_input.size(0) - b):\n",
    "                if data_target.data[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fully connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_net(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FC_net, self).__init__() \n",
    "        self.additional_hidden = nn.ModuleList()\n",
    "        for l in range(len(layers)-1):\n",
    "            self.additional_hidden.append(nn.Linear(layers[l], layers[l+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(x.shape[0], -1)\n",
    "        for l in range(len(self.additional_hidden)-1):\n",
    "            x = F.relu(self.additional_hidden[l](x))\n",
    "        x = self.additional_hidden[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check model parameters\n",
    "layers = [train_input.view(train_input.shape[0], -1).shape[1], 5, 5, 2]\n",
    "# for k in model_fc.parameters():\n",
    "#     print(k.size())\n",
    "parameters = {'type': 'fc', 'layers': layers}\n",
    "    \n",
    "mini_batch_size = 42\n",
    "nb_epochs = 10\n",
    "#costs, costs_val, acc, acc_val = train_model_full(parameters, train_input, train_target, kfolds, nb_epochs, lambdda=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "# ax1.plot (range(nb_epochs), costs)\n",
    "# ax1.plot (range(nb_epochs), costs_val)\n",
    "# ax2.plot (range(nb_epochs), acc)\n",
    "# ax2.plot (range(nb_epochs), acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('train_error {:.02f}%'.format(\n",
    "#            compute_nb_errors(model_fc, Variable(train_input), Variable(train_target), mini_batch_size = 79) / train_input.size(0) * 100))\n",
    "#print('test_error {:.02f}%'.format(\n",
    "#            compute_nb_errors(model_fc, test_input, test_target, mini_batch_size = 20) / test_input.size(0) * 100))\n",
    "\n",
    "# print(\"train data error = {}/316 %\".format(compute_nb_errors(model, train_input, train_target))\n",
    "# compute_nb_errors(model, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model few times and take average, because of different initialization\n",
    "#figure out the case when is 50% error for both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_net(nn.Module):\n",
    "    def __init__(self, layers, layers_conv, kernel_size, pooling_kernel_size, p):\n",
    "        super(Conv_net, self).__init__()\n",
    "        self.pooling_kernel_size = pooling_kernel_size\n",
    "        self.additional_conv_hidden = nn.ModuleList()\n",
    "        self.additional_fc_hidden = nn.ModuleList()\n",
    "        self.droput_layers = nn.ModuleList()\n",
    "        self.batch_normalization = nn.ModuleList()\n",
    "        \n",
    "        for l in range(len(layers_conv)-1):\n",
    "            self.additional_conv_hidden.append(nn.Conv1d(layers_conv[l], layers_conv[l+1], kernel_size=kernel_size[l]))\n",
    "            self.droput_layers.append(torch.nn.Dropout(p=p[l]))\n",
    "            self.batch_normalization.append(torch.nn.BatchNorm1d(layers_conv[l+1]))\n",
    "        size = train_input.shape[2]\n",
    "\n",
    "        for i in range(len(kernel_size)):\n",
    "            size-=(kernel_size[i]-1)\n",
    "\n",
    "            size//=pooling_kernel_size[i]\n",
    "\n",
    "        self.additional_fc_hidden.append(nn.Linear(size*layers_conv[-1], layers[0]))\n",
    "        self.droput_layers.append(torch.nn.Dropout(p=p[l+1]))\n",
    "        self.batch_normalization.append(torch.nn.BatchNorm1d(layers[0]))\n",
    "        self.flat_size = size*layers_conv[-1]\n",
    "        \n",
    "        start_p = l+2\n",
    "\n",
    "        for l in range(len(layers)-1):\n",
    "            self.additional_fc_hidden.append(nn.Linear(layers[l], layers[l+1]))\n",
    "            if l != len(layers)-2:\n",
    "                self.droput_layers.append(torch.nn.Dropout(p=p[l+start_p]))\n",
    "                self.batch_normalization.append(torch.nn.BatchNorm1d(layers[l+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in range(len(self.additional_conv_hidden)):\n",
    "            x = self.droput_layers[l](self.batch_normalization[l](F.relu(F.max_pool1d(self.additional_conv_hidden[l](x), \\\n",
    "                                                          kernel_size=self.pooling_kernel_size[l]))))\n",
    "        x=x.view(-1, self.flat_size)\n",
    "        for l in range(len(self.additional_fc_hidden)-1):\n",
    "            index = len(self.additional_conv_hidden)+l\n",
    "            x = self.droput_layers[index](self.batch_normalization[index](F.relu(self.additional_fc_hidden[l](x))))\n",
    "        x = self.additional_fc_hidden[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [0.2, 0.2, 0]\n",
    "layers = [5, 2]\n",
    "layers_conv = [28, 4, 4]\n",
    "kernel_size = [6, 6]\n",
    "pooling_kernel_size = [3, 2]\n",
    "# for k in model_conv.parameters():\n",
    "#     print(k.size())\n",
    "    \n",
    "parameters = {'type': 'conv', 'layers': layers, 'layers_conv':layers_conv, 'kernel_size': kernel_size, \\\n",
    "              'pooling_kernel_size': pooling_kernel_size, 'p_list': p_list}\n",
    "    \n",
    "mini_batch_size = 79\n",
    "nb_epochs = 500\n",
    "#costs, costs_val, acc, acc_val = train_model_full(parameters, train_input, train_target, kfolds, nb_epochs, lambdda=0.0375)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "# ax1.plot (range(nb_epochs), costs)\n",
    "# ax1.plot (range(nb_epochs), costs_val)\n",
    "# ax2.plot (range(nb_epochs), acc)\n",
    "# ax2.plot (range(nb_epochs), acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train_error {:.02f}%'.format(\n",
    "#             compute_nb_errors(model_conv, Variable(train_input), Variable(train_target), mini_batch_size = 79) / train_input.size(0) * 100))\n",
    "# print('test_error {:.02f}%'.format(\n",
    "#             compute_nb_errors(model_conv, test_input, test_target, mini_batch_size = 20) / test_input.size(0) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "     'lambda': np.linspace(0.01, 0.1, 30).tolist()+[0], \n",
    "     'lr': np.linspace(0.001, 0.1, 50).tolist()\n",
    "}\n",
    "nb_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parallel(param):\n",
    "    train_model_full(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [26, 36, 2]\n",
      "layers_conv: [28, 18, 9]\n",
      "kernel_size: [2, 6]\n",
      "    pooling_kernel_size: [3, 3]\n",
      "p: [0.5396743608694655, 0.2940088573782593, 0.5060165571226489, 0.19815867212709004]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [45, 20, 19, 2]\n",
      "layers_conv: [28, 3, 2, 12, 3, 12]\n",
      "kernel_size: [1, 3, 5, 5, 6]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 3]\n",
      "p: [0.07479414008746155, 0.6343406482691533, 0.7487105232357509, 0.8179817824447715, 0.4009548068246721, 0.6252312627953721, 0.11960620248438902, 0.4781655199005129]\n",
      "lambdda: 0.1\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 4\n",
      "layers_fc: [49, 2]\n",
      "layers_conv: [28, 5, 8, 19, 15]\n",
      "kernel_size: [5, 6, 5, 6]\n",
      "    pooling_kernel_size: [3, 3, 3, 2]\n",
      "p: [0.5834255933828782, 0.33761927476745013, 0.8917003666335612, 0.10659027381444752, 0.9681173846021117]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.015142857142857145\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [43, 25, 47, 2]\n",
      "layers_conv: [28, 8, 16, 11, 3, 6, 10, 13, 11, 6]\n",
      "kernel_size: [4, 5, 3, 6, 6, 2, 5, 2, 6]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 2, 3, 2, 3, 3]\n",
      "p: [0.3193863592315256, 0.26489554515271174, 0.2273090162417718, 0.4155431785151066, 0.35288062080616134, 0.4745222206935109, 0.6293048714038003, 0.2679693791449438, 0.7308536305234731, 0.15974937209139006, 0.007190433783397787, 0.17744750164541068]\n",
      "lambdda: 0\n",
      "lr: 0.0656530612244898\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [46, 20, 35, 2]\n",
      "layers_conv: [28, 15, 10, 11, 19]\n",
      "kernel_size: [5, 5, 3, 2]\n",
      "    pooling_kernel_size: [3, 3, 3, 3]\n",
      "p: [0.1329379777875478, 0.22347104774761406, 0.2584680677302539, 0.8989823155861791, 0.7975788783233075, 0.29833263862882875, 0.059249616266546457]\n",
      "lambdda: 0.08758620689655172\n",
      "lr: 0.059591836734693884\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [19, 2]\n",
      "layers_conv: [28, 5]\n",
      "kernel_size: [5]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.018237867016184572, 0.3372279101485872]\n",
      "lambdda: 0.04103448275862069\n",
      "lr: 0.0030204081632653063\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [38, 17, 2]\n",
      "layers_conv: [28, 16, 12, 3, 8, 14, 8, 10]\n",
      "kernel_size: [3, 2, 3, 5, 7, 6, 5]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2, 2, 2]\n",
      "p: [0.93008075242826, 0.2779992762801835, 0.38884766104261603, 0.06316626446247164, 0.8488768641068721, 0.598432686641536, 0.3398116442264907, 0.25301845827616565, 0.7057391642850834]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.059591836734693884\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 4\n",
      "layers_fc: [10, 6, 2]\n",
      "layers_conv: [28, 17, 15, 16, 3]\n",
      "kernel_size: [3, 1, 7, 7]\n",
      "    pooling_kernel_size: [3, 3, 2, 3]\n",
      "p: [0.24543104009839345, 0.6439063206207746, 0.9191920867586633, 0.6599687136012726, 0.1298889254323804, 0.8056939486094602]\n",
      "lambdda: 0.025517241379310347\n",
      "lr: 0.07373469387755102\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [22, 5, 2]\n",
      "layers_conv: [28, 16, 19]\n",
      "kernel_size: [5, 6]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.8047709027620965, 0.0684836070576128, 0.48654491872059547, 0.627283331612352]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.0050408163265306125\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [14, 49, 39, 2]\n",
      "layers_conv: [28, 2, 3, 5, 14]\n",
      "kernel_size: [6, 6, 5, 5]\n",
      "    pooling_kernel_size: [2, 2, 2, 3]\n",
      "p: [0.4405665501325966, 0.0803247332000131, 0.3336818330653225, 0.16164762896324647, 0.2768073921598754, 0.9945662859770373, 0.6173267339232432]\n",
      "lambdda: 0.07827586206896552\n",
      "lr: 0.011102040816326531\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [28, 42, 35, 2]\n",
      "layers_conv: [28, 7, 2, 11, 13, 13, 10, 9, 19, 4]\n",
      "kernel_size: [6, 6, 5, 4, 2, 3, 1, 1, 1]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 3, 3, 2, 3, 2]\n",
      "p: [0.6483709451520093, 0.38651225916059406, 0.1698167672023192, 0.8237858348689152, 0.8142202999847523, 0.06232298655404922, 0.878629487324765, 0.10574482072896008, 0.7604921391159386, 0.7640693315051361, 0.3158184779091784, 0.170097075877364]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.04948979591836735\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [37, 2]\n",
      "layers_conv: [28, 14, 16, 10, 3, 16, 2]\n",
      "kernel_size: [4, 7, 7, 3, 5, 5]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3, 3]\n",
      "p: [0.9302656430076572, 0.5925961215107479, 0.7327140598586857, 0.2076048175789672, 0.7124269034326817, 0.079571279001179, 0.7820274278782493]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.08787755102040817\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [40, 10, 42, 2]\n",
      "layers_conv: [28, 19, 3, 1, 10, 15, 2, 7]\n",
      "kernel_size: [4, 5, 6, 1, 1, 7, 7]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 2, 2, 3]\n",
      "p: [0.019211131127169878, 0.1824945977826865, 0.49650530236430235, 0.5286062747458671, 0.029598848599350136, 0.7763699821861204, 0.04013117790507026, 0.2515407084005956, 0.44297426171150467, 0.48923790558583236]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.05555102040816327\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [31, 19, 20, 2]\n",
      "layers_conv: [28, 16, 5]\n",
      "kernel_size: [5, 4]\n",
      "    pooling_kernel_size: [3, 2]\n",
      "p: [0.744031582668901, 0.5665982953443012, 0.4192236341311124, 0.36636815090485964, 0.9250276760633483]\n",
      "lambdda: 0.05655172413793104\n",
      "lr: 0.0050408163265306125\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [48, 34, 20, 2]\n",
      "layers_conv: [28, 15, 14]\n",
      "kernel_size: [1, 1]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.10738656287616244, 0.02581219653963751, 0.6273839293360471, 0.49127010185062736, 0.3585340500951879]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [49, 46, 34, 2]\n",
      "layers_conv: [28, 3, 6, 1, 3, 16, 11, 2]\n",
      "kernel_size: [7, 2, 6, 7, 4, 4, 1]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 3, 2, 3]\n",
      "p: [0.3620681992420821, 0.054547798813139314, 0.9634914728443014, 0.2781568605816014, 0.7480639162459389, 0.5909349636346266, 0.9305119337345548, 0.5688143577815682, 0.16616119102867755, 0.9583734079777514]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.0959591836734694\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [32, 10, 11, 2]\n",
      "layers_conv: [28, 5, 11, 10, 4, 16, 3]\n",
      "kernel_size: [1, 4, 7, 1, 1, 7]\n",
      "    pooling_kernel_size: [3, 2, 2, 3, 3, 3]\n",
      "p: [0.09730080031414068, 0.17524652623713155, 0.7580972692437769, 0.8718989614733356, 0.35830452464458595, 0.8150601814400982, 0.40707779851786896, 0.13909810081659646, 0.35283540022080684]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.043428571428571434\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [18, 21, 2]\n",
      "layers_conv: [28, 11, 8, 8]\n",
      "kernel_size: [3, 2, 7]\n",
      "    pooling_kernel_size: [2, 3, 3]\n",
      "p: [0.9032007755551895, 0.37751457848571535, 0.4872512808366998, 0.6085431739547542, 0.1815000397541301]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.047469387755102045\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [12, 10, 2]\n",
      "layers_conv: [28, 8, 16, 12]\n",
      "kernel_size: [7, 7, 1]\n",
      "    pooling_kernel_size: [2, 2, 3]\n",
      "p: [0.44052688462342926, 0.9551734868282391, 0.6653545490801929, 0.6557243901456483, 0.8167555456515434]\n",
      "lambdda: 0.07206896551724139\n",
      "lr: 0.011102040816326531\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [46, 13, 17, 2]\n",
      "layers_conv: [28, 1, 5]\n",
      "kernel_size: [5, 6]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.2805906066629523, 0.5897282437010412, 0.8461281834305102, 0.8861598975183774, 0.7654814129300109]\n",
      "lambdda: 0.1\n",
      "lr: 0.02322448979591837\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [9, 30, 12, 2]\n",
      "layers_conv: [28, 2]\n",
      "kernel_size: [1]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.26896540464390006, 0.16987750807369284, 0.06743827003416825, 0.7720698901965117]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.06161224489795919\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [45, 15, 2]\n",
      "layers_conv: [28, 6, 18, 18]\n",
      "kernel_size: [1, 4, 3]\n",
      "    pooling_kernel_size: [3, 2, 2]\n",
      "p: [0.11194258128025558, 0.2569214173097607, 0.271929272574263, 0.1587624098937921, 0.9750128873923474]\n",
      "lambdda: 0.04413793103448276\n",
      "lr: 0.02726530612244898\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [44, 5, 38, 2]\n",
      "layers_conv: [28, 7]\n",
      "kernel_size: [5]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.3408852069516265, 0.8420670765269003, 0.7912783652716192, 0.39478652966930694]\n",
      "lambdda: 0.04103448275862069\n",
      "lr: 0.02322448979591837\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [25, 35, 2]\n",
      "layers_conv: [28, 5, 12, 10, 13, 4, 7]\n",
      "kernel_size: [1, 6, 6, 6, 3, 2]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3, 2]\n",
      "p: [0.2242205942275206, 0.3189077684931323, 0.7307807033125674, 0.508840303371232, 0.5116699306355451, 0.5605364422798444, 0.8509033711654864, 0.6708325986191304]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.011102040816326531\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [18, 2, 28, 2]\n",
      "layers_conv: [28, 4, 3, 2, 5, 13, 13, 16]\n",
      "kernel_size: [1, 2, 5, 4, 2, 5, 1]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 3, 3, 3]\n",
      "p: [0.5766755347612567, 0.4833821667100773, 0.8449547241293737, 0.48395887690914297, 0.4315792317610422, 0.21427620911800316, 0.332810145122573, 0.35408518562048374, 0.7651324264797666, 0.9719200015431013]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.06969387755102041\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 4\n",
      "layers_fc: [6, 20, 2]\n",
      "layers_conv: [28, 11, 4, 4, 13]\n",
      "kernel_size: [2, 3, 2, 4]\n",
      "    pooling_kernel_size: [3, 3, 2, 2]\n",
      "p: [0.16264532663095654, 0.477144638053007, 0.06183380263591254, 0.8070811322834606, 0.35895814131422965, 0.5352894558809409]\n",
      "lambdda: 0.025517241379310347\n",
      "lr: 0.03736734693877551\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [8, 2]\n",
      "layers_conv: [28, 5, 11, 16, 17, 7, 15]\n",
      "kernel_size: [4, 1, 6, 1, 6, 2]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 2, 3]\n",
      "p: [0.30098994261729595, 0.8625039383123999, 0.7504287603543299, 0.09907207184311972, 0.8940458126164781, 0.5062717647301598, 0.18640565262713693]\n",
      "lambdda: 0.01310344827586207\n",
      "lr: 0.031306122448979595\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [18, 40, 1, 2]\n",
      "layers_conv: [28, 10, 18, 11, 3, 18, 13, 14, 11, 11]\n",
      "kernel_size: [7, 2, 5, 7, 7, 6, 6, 3, 7]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 3, 2, 3, 2]\n",
      "p: [0.7785704301367172, 0.6060677107733581, 0.5414698731129479, 0.29172155704110325, 0.6664972427017386, 0.7068145460216299, 0.6676445991236114, 0.3084023870038588, 0.9868833976309889, 0.8920008713962055, 0.9350571588246706, 0.7504012897821494]\n",
      "lambdda: 0.1\n",
      "lr: 0.05353061224489796\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [5, 9, 7, 2]\n",
      "layers_conv: [28, 9]\n",
      "kernel_size: [4]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.8119296918861442, 0.6632838822943644, 0.7305462909480852, 0.648729285456618]\n",
      "lambdda: 0.019310344827586208\n",
      "lr: 0.02928571428571429\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 8\n",
      "layers_fc: [43, 19, 8, 2]\n",
      "layers_conv: [28, 12, 8, 1, 9, 12, 17, 9, 9]\n",
      "kernel_size: [2, 3, 6, 2, 6, 6, 5, 2]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3, 3, 3, 2]\n",
      "p: [0.9371918283191699, 0.1368501673739254, 0.12999297547155686, 0.2644394892338563, 0.20989880130905614, 0.32832433315358334, 0.05708492457530756, 0.7944669525077104, 0.7403648825613133, 0.9806461871308123, 0.18660480828726977]\n",
      "lambdda: 0.01310344827586207\n",
      "lr: 0.09191836734693878\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [39, 10, 30, 2]\n",
      "layers_conv: [28, 17, 19]\n",
      "kernel_size: [5, 1]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.4046427770770994, 0.20364630059803135, 0.22124470198187796, 0.253274887205842, 0.5444468908952124]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.08585714285714287\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [8, 28, 2]\n",
      "layers_conv: [28, 2, 11, 17]\n",
      "kernel_size: [5, 7, 4]\n",
      "    pooling_kernel_size: [2, 3, 2]\n",
      "p: [0.48117170235018525, 0.5896233790313105, 0.2929817240420619, 0.08307240089335366, 0.5432636065652191]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.047469387755102045\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [29, 33, 12, 2]\n",
      "layers_conv: [28, 13, 14]\n",
      "kernel_size: [6, 5]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.6216729908043147, 0.9603965653985183, 0.3661130498049844, 0.1633214984232476, 0.059873293360702684]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [8, 14, 29, 2]\n",
      "layers_conv: [28, 4, 7, 18, 11, 2, 1, 1, 18, 3]\n",
      "kernel_size: [2, 3, 6, 4, 7, 6, 7, 6, 1]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 3, 2, 2, 2, 2]\n",
      "p: [0.7276848805027756, 0.7555727838301025, 0.1571018905822512, 0.22041908992916992, 0.3064934078633874, 0.10132475425248011, 0.2578422645441857, 0.7112480141083397, 0.9807067376516233, 0.891630748540666, 0.18988342210227727, 0.5391501612558436]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.03938775510204082\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [38, 24, 36, 2]\n",
      "layers_conv: [28, 16, 12, 17]\n",
      "kernel_size: [1, 2, 5]\n",
      "    pooling_kernel_size: [2, 2, 2]\n",
      "p: [0.42825647445587434, 0.2453500488892889, 0.18865237004669566, 0.1744870785065662, 0.6345173329445133, 0.09066349641742422]\n",
      "lambdda: 0.06275862068965518\n",
      "lr: 0.07979591836734694\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 9\n",
      "layers_fc: [41, 35, 2]\n",
      "layers_conv: [28, 2, 19, 19, 16, 4, 11, 5, 6, 3]\n",
      "kernel_size: [2, 5, 6, 1, 7, 4, 6, 4, 1]\n",
      "    pooling_kernel_size: [3, 2, 3, 2, 2, 3, 3, 3, 3]\n",
      "p: [0.6366905965574964, 0.20372147713700184, 0.8568767707600576, 0.7239382617263124, 0.42112784518670676, 0.3859754418111372, 0.41191090938760044, 0.21664973448465885, 0.8105997654163407, 0.7949650003937453, 0.5895605991237882]\n",
      "lambdda: 0.07827586206896552\n",
      "lr: 0.08787755102040817\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 8\n",
      "layers_fc: [13, 20, 2]\n",
      "layers_conv: [28, 12, 3, 18, 15, 19, 8, 16, 10]\n",
      "kernel_size: [6, 6, 4, 2, 1, 4, 3, 2]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 3, 2, 2]\n",
      "p: [0.4983853922889373, 0.4685941613074017, 0.0348007574842637, 0.29570937246007334, 0.6754291338732599, 0.7589588313211967, 0.2778930485532737, 0.21889689650399324, 0.0015285360056606079, 0.14329493724102071]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.06969387755102041\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 2\n",
      "layers_fc: [6, 2]\n",
      "layers_conv: [28, 12, 1]\n",
      "kernel_size: [4, 4]\n",
      "    pooling_kernel_size: [3, 3]\n",
      "p: [0.13722762904658692, 0.006377021866974308, 0.9346476762077035]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.047469387755102045\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [37, 46, 25, 2]\n",
      "layers_conv: [28, 1]\n",
      "kernel_size: [5]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.3248353539078229, 0.34415607404004145, 0.9222057680029001, 0.8462633613220873]\n",
      "lambdda: 0.04413793103448276\n",
      "lr: 0.03938775510204082\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [38, 12, 2]\n",
      "layers_conv: [28, 6, 9, 9, 15, 5, 4]\n",
      "kernel_size: [2, 2, 5, 3, 4, 4]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 2, 3]\n",
      "p: [0.7914787405483054, 0.8111709684610114, 0.477377349064523, 0.4209683686697574, 0.037152277411280576, 0.3434121507045691, 0.7276010727488305, 0.9627134465527587]\n",
      "lambdda: 0.08758620689655172\n",
      "lr: 0.03736734693877551\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [43, 21, 30, 2]\n",
      "layers_conv: [28, 11, 16, 19]\n",
      "kernel_size: [5, 6, 7]\n",
      "    pooling_kernel_size: [3, 3, 2]\n",
      "p: [0.14441139400142555, 0.14621367363873017, 0.08423059704878755, 0.5509627367369279, 0.39764396622540576, 0.7354573273450472]\n",
      "lambdda: 0.019310344827586208\n",
      "lr: 0.02120408163265306\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [4, 21, 34, 2]\n",
      "layers_conv: [28, 7, 5, 5, 10, 12, 14]\n",
      "kernel_size: [4, 1, 3, 1, 3, 6]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 2]\n",
      "p: [0.42860844367489404, 0.8140322998250753, 0.2583712347876327, 0.23171905911005108, 0.31599078565127625, 0.88096498870205, 0.3782826211260151, 0.603621814559034, 0.2450792373627041]\n",
      "lambdda: 0.1\n",
      "lr: 0.02726530612244898\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [32, 45, 40, 2]\n",
      "layers_conv: [28, 3, 19, 12]\n",
      "kernel_size: [6, 2, 6]\n",
      "    pooling_kernel_size: [3, 3, 2]\n",
      "p: [0.7261852870589472, 0.1773219406654063, 0.4383856221781398, 0.8254784931108939, 0.14308960527246117, 0.4373610658198862]\n",
      "lambdda: 0.07206896551724139\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [29, 37, 13, 2]\n",
      "layers_conv: [28, 7, 18, 13, 15, 19, 4]\n",
      "kernel_size: [4, 3, 6, 1, 6, 6]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2, 2]\n",
      "p: [0.045247177573417674, 0.8078536677645219, 0.6679710205634539, 0.07061219627082438, 0.8847628808857528, 0.5090040133057357, 0.3993208540006655, 0.9482490787840255, 0.11997670579533148]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.019183673469387756\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [10, 2]\n",
      "layers_conv: [28, 2]\n",
      "kernel_size: [3]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.32240120993529264, 0.723896217059204]\n",
      "lambdda: 0.031724137931034485\n",
      "lr: 0.0333265306122449\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 9\n",
      "layers_fc: [17, 2]\n",
      "layers_conv: [28, 16, 2, 4, 1, 2, 7, 10, 8, 11]\n",
      "kernel_size: [6, 4, 7, 4, 4, 2, 3, 5, 4]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 2, 2, 2, 3, 3]\n",
      "p: [0.4497651954362012, 0.7394985064918805, 0.7658165211069312, 0.22204812292255804, 0.7146746177344548, 0.2732365022669575, 0.5921171725693987, 0.4847194273967813, 0.6980976437896284, 0.6065393322402525]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.02120408163265306\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [26, 29, 2]\n",
      "layers_conv: [28, 2, 2]\n",
      "kernel_size: [2, 4]\n",
      "    pooling_kernel_size: [3, 3]\n",
      "p: [0.7405035657521578, 0.1639613979025678, 0.10750774405682983, 0.0802638495510054]\n",
      "lambdda: 0.04103448275862069\n",
      "lr: 0.051510204081632656\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [22, 2]\n",
      "layers_conv: [28, 17, 9, 12, 16, 8, 18]\n",
      "kernel_size: [1, 4, 3, 3, 4, 2]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3, 2]\n",
      "p: [0.9210819907323818, 0.5631187887903331, 0.7301742351813533, 0.9544448612078619, 0.2359907088155352, 0.023885930259088695, 0.9850392801803932]\n",
      "lambdda: 0.07206896551724139\n",
      "lr: 0.009081632653061226\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [21, 1, 2]\n",
      "layers_conv: [28, 2]\n",
      "kernel_size: [1]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.9036639322049816, 0.47012883238089953, 0.023878536536268258]\n",
      "lambdda: 0.08448275862068966\n",
      "lr: 0.0979795918367347\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [43, 2]\n",
      "layers_conv: [28, 4, 16, 16, 16, 1]\n",
      "kernel_size: [3, 6, 6, 5, 1]\n",
      "    pooling_kernel_size: [3, 2, 2, 2, 3]\n",
      "p: [0.5682585297974263, 0.3229927688453952, 0.8051099160734014, 0.535676099801314, 0.5394521620722834, 0.6271751083647673]\n",
      "lambdda: 0.0813793103448276\n",
      "lr: 0.04140816326530612\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [4, 12, 2]\n",
      "layers_conv: [28, 12, 13, 3, 10, 19]\n",
      "kernel_size: [2, 1, 6, 2, 5]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3]\n",
      "p: [0.15830918462730115, 0.08735699346143322, 0.4788440473335147, 0.162207204212799, 0.11588620512251746, 0.745111579686448, 0.4325266760645119]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [49, 12, 20, 2]\n",
      "layers_conv: [28, 16, 12, 1, 18, 15, 2, 1, 16, 4]\n",
      "kernel_size: [1, 5, 3, 7, 5, 4, 7, 1, 2]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 2, 2, 2, 3, 2]\n",
      "p: [0.02951866272658532, 0.8779688223608805, 0.959432023679848, 0.02071813076012685, 0.8387610574220848, 0.1914956303014389, 0.4836344296138043, 0.17942885461218616, 0.9876147551403801, 0.9338648633698435, 0.7922439880354878, 0.7597250881430201]\n",
      "lambdda: 0.08758620689655172\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [15, 46, 38, 2]\n",
      "layers_conv: [28, 11, 17, 11, 10, 15]\n",
      "kernel_size: [3, 3, 3, 4, 5]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 2]\n",
      "p: [0.6360043258503202, 0.9848293242080375, 0.9609444382374288, 0.03304318013779406, 0.6535635163770358, 0.7163140731623234, 0.9810610492329664, 0.8153461219723784]\n",
      "lambdda: 0.019310344827586208\n",
      "lr: 0.0030204081632653063\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [48, 32, 2]\n",
      "layers_conv: [28, 9, 17, 14, 12, 12]\n",
      "kernel_size: [7, 6, 5, 7, 3]\n",
      "    pooling_kernel_size: [2, 3, 3, 3, 2]\n",
      "p: [0.3324688920125721, 0.5729692237534123, 0.014337212305677638, 0.7475654601333686, 0.6613026616837333, 0.9960569789442073, 0.8755586595473871]\n",
      "lambdda: 0.07206896551724139\n",
      "lr: 0.0979795918367347\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [16, 49, 2]\n",
      "layers_conv: [28, 7, 14, 17, 13, 10]\n",
      "kernel_size: [7, 1, 6, 1, 3]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 2]\n",
      "p: [0.2508124261230317, 0.8584968014629093, 0.9159232541887606, 0.03596321726284313, 0.7668843325657178, 0.48958812099987825, 0.8635564742002595]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.051510204081632656\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [38, 4, 31, 2]\n",
      "layers_conv: [28, 8, 18]\n",
      "kernel_size: [7, 6]\n",
      "    pooling_kernel_size: [3, 2]\n",
      "p: [0.07987289385341234, 0.6664558047231658, 0.17856382205285315, 0.5196070063420078, 0.8964232744905997]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [41, 3, 2]\n",
      "layers_conv: [28, 15, 6, 10, 4, 7, 15]\n",
      "kernel_size: [1, 4, 4, 5, 1, 2]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 2, 3]\n",
      "p: [0.8657485493899684, 0.1472607820877524, 0.4118266746033171, 0.9702121756148003, 0.33655306430806253, 0.5998197110168972, 0.5617175919248292, 0.4204288372214342]\n",
      "lambdda: 0.031724137931034485\n",
      "lr: 0.05757142857142858\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 9\n",
      "layers_fc: [8, 2]\n",
      "layers_conv: [28, 12, 1, 5, 2, 4, 16, 16, 8, 19]\n",
      "kernel_size: [4, 7, 1, 7, 1, 7, 1, 2, 2]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3, 3, 3, 3, 3]\n",
      "p: [0.800616446154239, 0.4459052482140041, 0.5245459843263665, 0.2842953927424793, 0.8220995802117482, 0.7118465677478585, 0.8271661696011148, 0.309953608658186, 0.25204075207255106, 0.4559752156217366]\n",
      "lambdda: 0.04724137931034483\n",
      "lr: 0.04948979591836735\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 2\n",
      "layers_fc: [19, 2]\n",
      "layers_conv: [28, 10, 12]\n",
      "kernel_size: [3, 2]\n",
      "    pooling_kernel_size: [3, 3]\n",
      "p: [0.22468637788315127, 0.929183983952333, 0.706605040848088]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.009081632653061226\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [33, 16, 2]\n",
      "layers_conv: [28, 13, 5]\n",
      "kernel_size: [2, 4]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.30433833386429443, 0.794731450692919, 0.06461461837559845, 0.4433483998388583]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.015142857142857145\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [19, 2]\n",
      "layers_conv: [28, 4]\n",
      "kernel_size: [1]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.04940991616953738, 0.4917692584697415]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.02928571428571429\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 8\n",
      "layers_fc: [49, 4, 14, 2]\n",
      "layers_conv: [28, 13, 11, 10, 14, 7, 17, 10, 18]\n",
      "kernel_size: [4, 3, 5, 4, 1, 3, 6, 7]\n",
      "    pooling_kernel_size: [3, 2, 3, 2, 2, 3, 2, 2]\n",
      "p: [0.8955914912271065, 0.8025306309519501, 0.15345903867127908, 0.6384054219943714, 0.9534166196725644, 0.04180926480688596, 0.6722357319859984, 0.5256657294900053, 0.19095832414469982, 0.05678845436558844, 0.04967088030542166]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.04140816326530612\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [49, 47, 27, 2]\n",
      "layers_conv: [28, 17, 18]\n",
      "kernel_size: [3, 6]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.5064796333575083, 0.05931749877269388, 0.9031594806503166, 0.4029628670439044, 0.9934815512914197]\n",
      "lambdda: 0.05655172413793104\n",
      "lr: 0.06161224489795919\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [9, 2]\n",
      "layers_conv: [28, 8, 18, 12, 8, 13]\n",
      "kernel_size: [6, 6, 1, 6, 2]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 3]\n",
      "p: [0.11666704882980805, 0.2122274827391727, 0.9618083414815672, 0.4919334894583596, 0.9802134163132957, 0.3342659119330833]\n",
      "lambdda: 0.0813793103448276\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [48, 43, 18, 2]\n",
      "layers_conv: [28, 1, 2, 15]\n",
      "kernel_size: [6, 2, 3]\n",
      "    pooling_kernel_size: [3, 2, 3]\n",
      "p: [0.5331654656596343, 0.06127426911492839, 0.38411400203258006, 0.20405000098949266, 0.9785614697730299, 0.43125033365824517]\n",
      "lambdda: 0.09379310344827586\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 8\n",
      "layers_fc: [49, 12, 49, 2]\n",
      "layers_conv: [28, 1, 6, 15, 7, 19, 16, 15, 7]\n",
      "kernel_size: [4, 1, 2, 1, 5, 4, 5, 7]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 2, 3, 3, 3]\n",
      "p: [0.6282220522467294, 0.7145502890050532, 0.7113706156454204, 0.2480555030204491, 0.30207172104399715, 0.9157728802138169, 0.12562153903346118, 0.19915645484250033, 0.7969521866549028, 0.9434559822161784, 0.036380745719935104]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [29, 38, 2]\n",
      "layers_conv: [28, 11, 10, 19]\n",
      "kernel_size: [6, 2, 1]\n",
      "    pooling_kernel_size: [3, 3, 2]\n",
      "p: [0.18982752706272032, 0.404455077835153, 0.10444567032913821, 0.6353207180654562, 0.1254149846306326]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.02120408163265306\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [22, 4, 29, 2]\n",
      "layers_conv: [28, 15, 4, 2, 19, 8, 7, 1]\n",
      "kernel_size: [3, 4, 5, 5, 4, 5, 6]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 2, 3, 2]\n",
      "p: [0.7618350765088777, 0.43323291709935974, 0.6174786364979457, 0.41681750876509527, 0.4457622251513762, 0.05817573911072371, 0.430049459989989, 0.6851089638742736, 0.09977358846626327, 0.5374288379097085]\n",
      "lambdda: 0.07827586206896552\n",
      "lr: 0.04140816326530612\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [49, 40, 2]\n",
      "layers_conv: [28, 16, 10, 10, 9, 16, 19, 13]\n",
      "kernel_size: [2, 3, 4, 7, 6, 6, 5]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 2, 3, 2]\n",
      "p: [0.3692571767625218, 0.8891351859778425, 0.12094821395786659, 0.6481236110518673, 0.8386285256631621, 0.6044319334662434, 0.18883920134982735, 0.6526061181832914, 0.5657614461474173]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.07373469387755102\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [29, 10, 34, 2]\n",
      "layers_conv: [28, 5]\n",
      "kernel_size: [7]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.506199732972089, 0.6176390963560235, 0.6377925460379684, 0.08654161228752122]\n",
      "lambdda: 0\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [20, 23, 2]\n",
      "layers_conv: [28, 3]\n",
      "kernel_size: [1]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.9075626064227882, 0.5910883843813928, 0.7177830325496302]\n",
      "lambdda: 0.09379310344827586\n",
      "lr: 0.05757142857142858\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [41, 32, 49, 2]\n",
      "layers_conv: [28, 3, 8, 12, 15, 5]\n",
      "kernel_size: [2, 3, 2, 3, 3]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 3]\n",
      "p: [0.38316301007353437, 0.07722140790551313, 0.9239861434290779, 0.9047982900904624, 0.9183873354525407, 0.4524803761618079, 0.8618072900270773, 0.9100241448121031]\n",
      "lambdda: 0.01310344827586207\n",
      "lr: 0.05757142857142858\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [11, 2]\n",
      "layers_conv: [28, 10, 2, 10, 14, 11, 13]\n",
      "kernel_size: [4, 7, 2, 5, 5, 1]\n",
      "    pooling_kernel_size: [3, 2, 2, 2, 2, 3]\n",
      "p: [0.3010939633312474, 0.7453134714940369, 0.05521058359257447, 0.17306707790744957, 0.916557029106233, 0.8339198147594022, 0.20383008608245667]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.08989795918367348\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 8\n",
      "layers_fc: [13, 28, 35, 2]\n",
      "layers_conv: [28, 6, 7, 16, 7, 10, 8, 5, 10]\n",
      "kernel_size: [3, 5, 3, 3, 2, 6, 2, 2]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 3, 2, 2, 3]\n",
      "p: [0.8662234948394102, 0.5928308149092377, 0.4832281018285698, 0.652912433959897, 0.6153483389360237, 0.3817741055388, 0.9629944377092978, 0.3788244672277782, 0.4331253230180888, 0.5942772191471529, 0.557971257122963]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.051510204081632656\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [22, 2]\n",
      "layers_conv: [28, 11]\n",
      "kernel_size: [4]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.15771989822095578, 0.5407167074577094]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.1\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [5, 2]\n",
      "layers_conv: [28, 3, 5, 15, 12, 4, 5]\n",
      "kernel_size: [2, 6, 6, 6, 6, 3]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 2, 2]\n",
      "p: [0.18322625821997274, 0.6063533237961094, 0.993797071314776, 0.30443176168943675, 0.5797855364296158, 0.29184945932675044, 0.20913323860529565]\n",
      "lambdda: 0.06275862068965518\n",
      "lr: 0.015142857142857145\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [19, 2]\n",
      "layers_conv: [28, 16, 4, 10]\n",
      "kernel_size: [2, 4, 6]\n",
      "    pooling_kernel_size: [3, 3, 3]\n",
      "p: [0.31766252298954456, 0.6768763449077821, 0.8103090863362593, 0.6872902855339617]\n",
      "lambdda: 0.04724137931034483\n",
      "lr: 0.03938775510204082\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [22, 25, 2]\n",
      "layers_conv: [28, 12, 9, 15, 16, 4, 12, 4]\n",
      "kernel_size: [2, 4, 2, 7, 6, 1, 2]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2, 2, 2]\n",
      "p: [0.8862748056121916, 0.1020708788461886, 0.45915550736154387, 0.18315177007420713, 0.9609930471577028, 0.5095240388453498, 0.8806071016305151, 0.109859800761955, 0.2714514143783341]\n",
      "lambdda: 0.08758620689655172\n",
      "lr: 0.08383673469387756\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [10, 40, 2]\n",
      "layers_conv: [28, 5, 12, 14, 4, 7]\n",
      "kernel_size: [1, 2, 5, 1, 6]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 3]\n",
      "p: [0.4084473945011562, 0.6537486919718666, 0.1556237743346871, 0.3733711419306043, 0.3557472993578237, 0.85127640029341, 0.6962339780361088]\n",
      "lambdda: 0.04413793103448276\n",
      "lr: 0.02726530612244898\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [18, 13, 45, 2]\n",
      "layers_conv: [28, 10]\n",
      "kernel_size: [7]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.992820720411207, 0.7766038920649384, 0.15279494689729045, 0.38670690383780815]\n",
      "lambdda: 0.0813793103448276\n",
      "lr: 0.0979795918367347\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 2\n",
      "layers_fc: [40, 2]\n",
      "layers_conv: [28, 3, 17]\n",
      "kernel_size: [2, 4]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.05553328246429767, 0.2906304598884921, 0.24428761671549382]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.01716326530612245\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [25, 11, 6, 2]\n",
      "layers_conv: [28, 15, 9, 16, 16, 4, 10, 15, 6, 8]\n",
      "kernel_size: [6, 4, 3, 1, 6, 5, 1, 7, 1]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2, 3, 2, 3, 3]\n",
      "p: [0.3807812864404738, 0.208232554218011, 0.21020592294315243, 0.851251014230845, 0.15822912001558287, 0.8775134282694783, 0.6739553992399818, 0.9866845958514003, 0.35871110554961727, 0.7827852959889114, 0.8026440697286491, 0.6505272606927474]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.02120408163265306\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [4, 40, 23, 2]\n",
      "layers_conv: [28, 8, 14, 15, 17, 19]\n",
      "kernel_size: [2, 4, 2, 6, 1]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 2]\n",
      "p: [0.8740798372724727, 0.2755616484963548, 0.47121209762285143, 0.7098200731523793, 0.941303371593943, 0.47124605822769894, 0.19341329413696895, 0.8283289719815625]\n",
      "lambdda: 0.05655172413793104\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [43, 5, 2]\n",
      "layers_conv: [28, 7]\n",
      "kernel_size: [7]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.7059443297879586, 0.7723733797321339, 0.1636842307872859]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 8\n",
      "layers_fc: [11, 2]\n",
      "layers_conv: [28, 7, 1, 2, 3, 17, 14, 8, 10]\n",
      "kernel_size: [5, 7, 1, 1, 4, 4, 1, 2]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3, 3, 3, 2]\n",
      "p: [0.5501938570934903, 0.5709964600220953, 0.9808772572786151, 0.3051982707779697, 0.5775457425834114, 0.5513583770046135, 0.3742174437713759, 0.052261482655958535, 0.5561257319954802]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [35, 2]\n",
      "layers_conv: [28, 2, 14, 11, 14, 10]\n",
      "kernel_size: [6, 2, 1, 1, 2]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3]\n",
      "p: [0.9717451373786594, 0.5276827883573917, 0.2821111021822067, 0.9288616187660356, 0.2659156022594632, 0.7739105744423258]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.08181632653061224\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 9\n",
      "layers_fc: [23, 39, 2]\n",
      "layers_conv: [28, 5, 15, 18, 7, 1, 13, 10, 10, 10]\n",
      "kernel_size: [1, 5, 3, 4, 5, 5, 1, 2, 4]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3, 3, 2, 2, 2]\n",
      "p: [0.5478893508088566, 0.08333012790237915, 0.7421398108703934, 0.9489918039456733, 0.3137048904981449, 0.39420308202452936, 0.3347239403862946, 0.16751394751056126, 0.6820905018662614, 0.39153607122076595, 0.8679088884906283]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [27, 2]\n",
      "layers_conv: [28, 19, 16, 18, 18, 4]\n",
      "kernel_size: [1, 4, 2, 7, 2]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2]\n",
      "p: [0.3513144371126796, 0.140882621029648, 0.27749206494877665, 0.4906212501971472, 0.8172042523306164, 0.12975149288668064]\n",
      "lambdda: 0.05655172413793104\n",
      "lr: 0.013122448979591837\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [15, 30, 37, 2]\n",
      "layers_conv: [28, 16, 12, 11]\n",
      "kernel_size: [1, 4, 5]\n",
      "    pooling_kernel_size: [3, 2, 3]\n",
      "p: [0.42439111194006596, 0.8781594716639199, 0.8691012385676411, 0.6841476014317329, 0.9585502531383453, 0.8750358653300988]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.007061224489795919\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [8, 49, 2]\n",
      "layers_conv: [28, 5, 4, 5, 19, 7, 1, 19]\n",
      "kernel_size: [3, 2, 2, 6, 5, 6, 2]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 2, 3, 2]\n",
      "p: [0.6115519460971429, 0.866418065608576, 0.3752322814393434, 0.6760655528660525, 0.7064689334419898, 0.21867408509813024, 0.8614736475203497, 0.06521855038411095, 0.7926106211162607]\n",
      "lambdda: 0.025517241379310347\n",
      "lr: 0.05353061224489796\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [28, 1, 2]\n",
      "layers_conv: [28, 6, 5, 2, 6, 11, 5, 3]\n",
      "kernel_size: [4, 3, 5, 1, 3, 4, 3]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 2, 3, 2]\n",
      "p: [0.18418939619523378, 0.9961867577728585, 0.1392181391989309, 0.5391699617488155, 0.5534655784282492, 0.6386368047635175, 0.6823284153843712, 0.8658218496368961, 0.528080227451354]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.08181632653061224\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [9, 31, 31, 2]\n",
      "layers_conv: [28, 9, 6]\n",
      "kernel_size: [3, 4]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.1971489714994905, 0.44576942748806614, 0.6796952905099565, 0.6935754633689881, 0.040938711777749326]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.09191836734693878\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [34, 47, 2]\n",
      "layers_conv: [28, 6, 16, 18, 11, 9, 5]\n",
      "kernel_size: [5, 1, 7, 2, 2, 6]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 2, 3]\n",
      "p: [0.8058424334992766, 0.3316860641861711, 0.555696967005836, 0.22641870423166266, 0.9050592965317329, 0.5346396468089443, 0.12892074228804606, 0.5154154751501282]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.08787755102040817\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 8\n",
      "layers_fc: [38, 2]\n",
      "layers_conv: [28, 4, 14, 13, 13, 17, 12, 16, 3]\n",
      "kernel_size: [7, 1, 1, 3, 7, 1, 5, 1]\n",
      "    pooling_kernel_size: [2, 2, 3, 2, 2, 3, 3, 3]\n",
      "p: [0.9696727879603932, 0.5198855356708048, 0.21584427308122434, 0.6848034953783028, 0.3361619451162111, 0.0822803897028076, 0.7413106439019842, 0.4537594717676561, 0.7020276569945477]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [31, 40, 14, 2]\n",
      "layers_conv: [28, 18, 1, 9, 9, 1, 4, 15]\n",
      "kernel_size: [1, 4, 2, 2, 3, 4, 1]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3, 3, 2]\n",
      "p: [0.7327895943134378, 0.33379169370365047, 0.39594181215538193, 0.9941255706591968, 0.6632812573224112, 0.886292503708289, 0.38387968613383316, 0.43384084209643736, 0.9890332718866325, 0.21242373844701434]\n",
      "lambdda: 0.04724137931034483\n",
      "lr: 0.03938775510204082\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 2\n",
      "layers_fc: [34, 28, 24, 2]\n",
      "layers_conv: [28, 3, 12]\n",
      "kernel_size: [1, 6]\n",
      "    pooling_kernel_size: [3, 2]\n",
      "p: [0.31547314834509854, 0.18645358453898697, 0.998623319455827, 0.8906038641168812, 0.021911405824224905]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.015142857142857145\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [13, 2]\n",
      "layers_conv: [28, 7]\n",
      "kernel_size: [5]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.8649326821899723, 0.9006452555430043]\n",
      "lambdda: 0.09379310344827586\n",
      "lr: 0.0030204081632653063\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [46, 27, 2]\n",
      "layers_conv: [28, 7, 17]\n",
      "kernel_size: [6, 1]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.46589010803877196, 0.42603294639852507, 0.6539449174905271, 0.5123094361642797]\n",
      "lambdda: 0.06275862068965518\n",
      "lr: 0.0030204081632653063\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [6, 2]\n",
      "layers_conv: [28, 16, 15, 5]\n",
      "kernel_size: [6, 3, 4]\n",
      "    pooling_kernel_size: [2, 2, 3]\n",
      "p: [0.30323324130344353, 0.3248323375279336, 0.2730414490330195, 0.23860231307614055]\n",
      "lambdda: 0.08448275862068966\n",
      "lr: 0.059591836734693884\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [38, 2]\n",
      "layers_conv: [28, 6, 18, 19]\n",
      "kernel_size: [1, 3, 6]\n",
      "    pooling_kernel_size: [2, 2, 3]\n",
      "p: [0.7486651091521516, 0.8681512987262751, 0.6215819368401075, 0.4234513467469888]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.011102040816326531\n"
     ]
    }
   ],
   "source": [
    "run_list = []\n",
    "for i in range (100):\n",
    "    try:\n",
    "        # param_num_layers_fc\n",
    "        num_layers_fc = np.random.randint(1, 4)\n",
    "        #param_num_layers_conv\n",
    "        num_layers_conv = np.random.randint(1, 10)\n",
    "        layers_fc = np.random.randint(1, 50, num_layers_fc).tolist()+[2]\n",
    "        layers_conv = [28] + np.random.randint(1,20, num_layers_conv).tolist()\n",
    "        kernel_size = np.random.randint(1, 8, num_layers_conv).tolist()\n",
    "        pooling_kernel_size = np.random.randint(2, 4, num_layers_conv).tolist()\n",
    "        p = np.random.rand(num_layers_fc+num_layers_conv).tolist()\n",
    "        lambdda = parameters['lambda'][np.random.randint(1, len(parameters['lambda']))]\n",
    "        lr = parameters['lr'][np.random.randint(1, len(parameters['lr']))]\n",
    "        print(\"num_layers_fc: {}\\nnum_layers_conv: {}\\nlayers_fc: {}\\nlayers_conv: {}\\nkernel_size: {}\\n\\\n",
    "    pooling_kernel_size: {}\\np: {}\\nlambdda: {}\\nlr: {}\"\\\n",
    "              .format(num_layers_fc, num_layers_conv, layers_fc, layers_conv, kernel_size, \\\n",
    "                      pooling_kernel_size,p, lambdda, lr))\n",
    "        param = {'type': 'conv', 'layers': layers_fc, 'layers_conv':layers_conv, 'kernel_size': kernel_size, \\\n",
    "                  'pooling_kernel_size': pooling_kernel_size, 'p_list': p}\n",
    "        run_params = {'param': param, 'train_input': train_input, 'train_target': train_target, 'kfolds': kfolds, 'nb_epochs': nb_epochs, 'lambdda': lambdda, 'lr': lr}\n",
    "        run_list.append(run_params)\n",
    "        #loss_train_kfold, loss_val_kfold, acc_train_kfold, acc_val_kfold = train_model_full(param, train_input, train_target, kfolds, nb_epochs, lambdda, lr)\n",
    "    except: pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e33584a55a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-25:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n",
      "Process ForkPoolWorker-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "pool.map(train_parallel, run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature with time\n",
    "#steap and flat for right/left\n",
    "#throw away some features, i.e. feature 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test import pr\n",
    "pool = Pool(4)\n",
    "pool.map(pr, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "2\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.504 \n",
      "   Loss: val ~ 0.696 / Acc val ~ 0.51\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.694 / Acc val ~ 0.5\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.505 \n",
      "   Loss: val ~ 0.696 / Acc val ~ 0.471\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.507 \n",
      "   Loss: val ~ 0.695 / Acc val ~ 0.484\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.498 \n",
      "   Loss: val ~ 0.693 / Acc val ~ 0.481\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.508 \n",
      "   Loss: val ~ 0.694 / Acc val ~ 0.465\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.695 / Acc val ~ 0.504\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.5 \n",
      "   Loss: val ~ 0.701 / Acc val ~ 0.471\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.695 / Acc val ~ 0.488\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.694 / Acc val ~ 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensor.c:24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/Users/ana/Desktop/deeplearning/proj1-dl/src/proj1-dl/Project.py\", line 366, in train_parallel\n    train_model_full(**param)\n  File \"/Users/ana/Desktop/deeplearning/proj1-dl/src/proj1-dl/Project.py\", line 86, in train_model_full\n    elif param['type']=='conv': model = model_conv = Conv_net(param['layers'], param['layers_conv'],                                                     param['kernel_size'], param['pooling_kernel_size'], param['p_list'])\n  File \"/Users/ana/Desktop/deeplearning/proj1-dl/src/proj1-dl/Project.py\", line 285, in __init__\n    self.additional_fc_hidden.append(nn.Linear(size*layers_conv[-1], layers[0]))\n  File \"/Users/ana/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 46, in __init__\n    self.reset_parameters()\n  File \"/Users/ana/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 49, in reset_parameters\n    stdv = 1. / math.sqrt(self.weight.size(1))\nRuntimeError: invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensor.c:24\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-79934fe54f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensor.c:24"
     ]
    }
   ],
   "source": [
    "from Project import train_parallel, run_list\n",
    "pool = Pool(4)\n",
    "pool.map(pr, [1,2,3])\n",
    "\n",
    "pool.map(train_parallel, run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
