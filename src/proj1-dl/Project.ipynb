{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import dlc_bci as bci\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = bci.load(root = './data_bci')\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "X = train_input.numpy()\n",
    "y = train_target.numpy()\n",
    "kfolds = model_selection.KFold(n_splits=10, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is done!\n"
     ]
    }
   ],
   "source": [
    "# put this inside the train to avoid data snooping\n",
    "mu, std = train_input.mean(0), train_input.std(0)\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "print(\"Normalization is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input  = Variable(test_input)\n",
    "test_target = Variable(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_full(param, train_input, train_target, kfolds, nb_epochs, lambdda = 0.01, lr = 0.001):\n",
    "    \n",
    "    acc_train_kfold = []\n",
    "    loss_train_kfold = []\n",
    "    acc_val_kfold = []\n",
    "    loss_val_kfold = []\n",
    "    \n",
    "    for train_index, val_index in kfolds.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_train = Variable(torch.from_numpy(X_train))\n",
    "        X_val = Variable(torch.from_numpy(X_val))\n",
    "        y_train = Variable(torch.from_numpy(y_train))\n",
    "        y_val = Variable(torch.from_numpy(y_val)) \n",
    "        \n",
    "        if param['type']=='fc': model = FC_net(param['layers'])\n",
    "        elif param['type']=='conv': model = model_conv = Conv_net(param['layers'], param['layers_conv'], \\\n",
    "                                                    param['kernel_size'], param['pooling_kernel_size'], param['p_list'])\n",
    "        \n",
    "        loss_train, loss_val, acc_train, acc_val = train_model(model, X_train, y_train, X_val, y_val, kfolds, nb_epochs, lambdda, lr)\n",
    "        acc_train_kfold.append(acc_train)\n",
    "        loss_train_kfold.append(loss_train)\n",
    "        acc_val_kfold.append(acc_val)\n",
    "        loss_val_kfold.append(loss_val)\n",
    "        \n",
    "    acc_train_kfold = np.mean(np.array(acc_train_kfold), axis=0)\n",
    "    acc_val_kfold = np.mean(np.array(acc_val_kfold), axis=0)\n",
    "\n",
    "    loss_train_kfold = np.mean(np.array(loss_train_kfold), axis=0)\n",
    "    loss_val_kfold = np.mean(np.array(loss_val_kfold), axis=0)\n",
    "    \n",
    "    print('\\n\\n---- Epochs Done -----\\n')\n",
    "    print('Loss: train ~ {} Acc train ~ {} \\n   Loss: val ~ {} / Acc val ~ {}\\n'\n",
    "         .format(round(loss_train_kfold[-1], 3), \n",
    "                 round(acc_train_kfold[-1], 3), \n",
    "                 round(loss_val_kfold[-1], 3), \n",
    "                 round(acc_val_kfold[-1], 3)))\n",
    "    return loss_train_kfold, loss_val_kfold, acc_train_kfold, acc_val_kfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, kfolds, nb_epochs, lambdda = 0.01, lr = 0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "        \n",
    "    for e in range(0, nb_epochs):\n",
    "         \n",
    "        model.train(True)\n",
    "        for b in list(range(0, X_train.size(0), mini_batch_size)):\n",
    "            if b + mini_batch_size <= X_train.size(0):\n",
    "                output = model(X_train.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, y_train.narrow(0, b, mini_batch_size))\n",
    "            else:\n",
    "                output = model(X_train.narrow(0, b, X_train.size(0) - b))\n",
    "                loss = criterion(output, y_train.narrow(0, b, X_train.size(0) - b))\n",
    "\n",
    "            for p in model.parameters():\n",
    "                loss += lambdda*p.pow(2).sum()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            model.train(False)\n",
    "            output_train = model(X_train)\n",
    "            output_val = model(X_val)\n",
    "            \n",
    "        acc_val.append(1-compute_nb_errors(model, X_val, y_val, mini_batch_size=mini_batch_size)/X_val.size(0))\n",
    "        acc_train.append(1-compute_nb_errors(model, X_train, y_train, mini_batch_size=mini_batch_size)/X_train.size(0))\n",
    "        loss_train.append(criterion(output_train, y_train).data[0])\n",
    "        loss_val.append(criterion(output_val, y_val).data[0])\n",
    "        \n",
    "#         if (e % 100 == 0):\n",
    "#                 print('Epoch {}: \\n   CVLoss: train ~ {} CVAcc train ~ {} \\n   CVLoss: val ~ {} / CVAcc val ~ {}'\n",
    "#                   .format(e, \n",
    "#                           round(loss_train_epoch[-1], 3),\n",
    "#                           round(acc_train_epoch[-1], 3), \n",
    "#                           round(loss_val_epoch[-1], 3),\n",
    "#                           round(acc_val_epoch[-1], 3)))\n",
    "        \n",
    "#     print('\\n\\n---- Epochs Done -----\\n')\n",
    "#     print('CVLoss: train ~ {} CVAcc train ~ {} \\n   CVLoss: val ~ {} / CVAcc val ~ {}'\n",
    "#          .format(round(loss_train_epoch[-1], 3), \n",
    "#                  round(acc_train_epoch[-1], 3), \n",
    "#                  round(loss_val_epoch[-1], 3), \n",
    "#                  round(acc_val_epoch[-1], 3)))\n",
    "\n",
    "    return loss_train, loss_val, acc_train, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        if b + mini_batch_size <= data_input.size(0):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = torch.max(output.data, 1)\n",
    "            for k in range(0, mini_batch_size):\n",
    "                if data_target.data[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "        else:       \n",
    "            output = model(data_input.narrow(0, b, data_input.size(0) - b))\n",
    "            _, predicted_classes = torch.max(output.data, 1)\n",
    "            for k in range(0, data_input.size(0) - b):\n",
    "                if data_target.data[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fully connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_net(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FC_net, self).__init__() \n",
    "        self.additional_hidden = nn.ModuleList()\n",
    "        for l in range(len(layers)-1):\n",
    "            self.additional_hidden.append(nn.Linear(layers[l], layers[l+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(x.shape[0], -1)\n",
    "        for l in range(len(self.additional_hidden)-1):\n",
    "            x = F.relu(self.additional_hidden[l](x))\n",
    "        x = self.additional_hidden[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check model parameters\n",
    "layers = [train_input.view(train_input.shape[0], -1).shape[1], 5, 5, 2]\n",
    "# for k in model_fc.parameters():\n",
    "#     print(k.size())\n",
    "parameters = {'type': 'fc', 'layers': layers}\n",
    "    \n",
    "mini_batch_size = 42\n",
    "nb_epochs = 10\n",
    "#costs, costs_val, acc, acc_val = train_model_full(parameters, train_input, train_target, kfolds, nb_epochs, lambdda=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "# ax1.plot (range(nb_epochs), costs)\n",
    "# ax1.plot (range(nb_epochs), costs_val)\n",
    "# ax2.plot (range(nb_epochs), acc)\n",
    "# ax2.plot (range(nb_epochs), acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('train_error {:.02f}%'.format(\n",
    "#            compute_nb_errors(model_fc, Variable(train_input), Variable(train_target), mini_batch_size = 79) / train_input.size(0) * 100))\n",
    "#print('test_error {:.02f}%'.format(\n",
    "#            compute_nb_errors(model_fc, test_input, test_target, mini_batch_size = 20) / test_input.size(0) * 100))\n",
    "\n",
    "# print(\"train data error = {}/316 %\".format(compute_nb_errors(model, train_input, train_target))\n",
    "# compute_nb_errors(model, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model few times and take average, because of different initialization\n",
    "#figure out the case when is 50% error for both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_net(nn.Module):\n",
    "    def __init__(self, layers, layers_conv, kernel_size, pooling_kernel_size, p):\n",
    "        super(Conv_net, self).__init__()\n",
    "        self.pooling_kernel_size = pooling_kernel_size\n",
    "        self.additional_conv_hidden = nn.ModuleList()\n",
    "        self.additional_fc_hidden = nn.ModuleList()\n",
    "        self.droput_layers = nn.ModuleList()\n",
    "        self.batch_normalization = nn.ModuleList()\n",
    "        \n",
    "        for l in range(len(layers_conv)-1):\n",
    "            self.additional_conv_hidden.append(nn.Conv1d(layers_conv[l], layers_conv[l+1], kernel_size=kernel_size[l]))\n",
    "            self.droput_layers.append(torch.nn.Dropout(p=p[l]))\n",
    "            self.batch_normalization.append(torch.nn.BatchNorm1d(layers_conv[l+1]))\n",
    "        size = train_input.shape[2]\n",
    "\n",
    "        for i in range(len(kernel_size)):\n",
    "            size-=(kernel_size[i]-1)\n",
    "\n",
    "            size//=pooling_kernel_size[i]\n",
    "\n",
    "        self.additional_fc_hidden.append(nn.Linear(size*layers_conv[-1], layers[0]))\n",
    "        self.droput_layers.append(torch.nn.Dropout(p=p[l+1]))\n",
    "        self.batch_normalization.append(torch.nn.BatchNorm1d(layers[0]))\n",
    "        self.flat_size = size*layers_conv[-1]\n",
    "        \n",
    "        start_p = l+2\n",
    "\n",
    "        for l in range(len(layers)-1):\n",
    "            self.additional_fc_hidden.append(nn.Linear(layers[l], layers[l+1]))\n",
    "            if l != len(layers)-2:\n",
    "                self.droput_layers.append(torch.nn.Dropout(p=p[l+start_p]))\n",
    "                self.batch_normalization.append(torch.nn.BatchNorm1d(layers[l+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in range(len(self.additional_conv_hidden)):\n",
    "            x = self.droput_layers[l](self.batch_normalization[l](F.relu(F.max_pool1d(self.additional_conv_hidden[l](x), \\\n",
    "                                                          kernel_size=self.pooling_kernel_size[l]))))\n",
    "        x=x.view(-1, self.flat_size)\n",
    "        for l in range(len(self.additional_fc_hidden)-1):\n",
    "            index = len(self.additional_conv_hidden)+l\n",
    "            x = self.droput_layers[index](self.batch_normalization[index](F.relu(self.additional_fc_hidden[l](x))))\n",
    "        x = self.additional_fc_hidden[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [0.2, 0.2, 0]\n",
    "layers = [5, 2]\n",
    "layers_conv = [28, 4, 4]\n",
    "kernel_size = [6, 6]\n",
    "pooling_kernel_size = [3, 2]\n",
    "# for k in model_conv.parameters():\n",
    "#     print(k.size())\n",
    "    \n",
    "parameters = {'type': 'conv', 'layers': layers, 'layers_conv':layers_conv, 'kernel_size': kernel_size, \\\n",
    "              'pooling_kernel_size': pooling_kernel_size, 'p_list': p_list}\n",
    "    \n",
    "mini_batch_size = 79\n",
    "#nb_epochs = 500\n",
    "#costs, costs_val, acc, acc_val = train_model_full(parameters, train_input, train_target, kfolds, nb_epochs, lambdda=0.0375)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "# ax1.plot (range(nb_epochs), costs)\n",
    "# ax1.plot (range(nb_epochs), costs_val)\n",
    "# ax2.plot (range(nb_epochs), acc)\n",
    "# ax2.plot (range(nb_epochs), acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train_error {:.02f}%'.format(\n",
    "#             compute_nb_errors(model_conv, Variable(train_input), Variable(train_target), mini_batch_size = 79) / train_input.size(0) * 100))\n",
    "# print('test_error {:.02f}%'.format(\n",
    "#             compute_nb_errors(model_conv, test_input, test_target, mini_batch_size = 20) / test_input.size(0) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "     'lambda': np.linspace(0.01, 0.1, 30).tolist()+[0], \n",
    "     'lr': np.linspace(0.001, 0.1, 50).tolist()\n",
    "}\n",
    "nb_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (100):\n",
    "    try:\n",
    "        # param_num_layers_fc\n",
    "        num_layers_fc = np.random.randint(1, 10)\n",
    "        #param_num_layers_conv\n",
    "        num_layers_conv = np.random.randint(1, 10)\n",
    "        layers_fc = np.random.randint(1, 50, num_layers_fc).tolist()+[2]\n",
    "        layers_conv = [28] + np.random.randint(1,20, num_layers_conv).tolist()\n",
    "        kernel_size = np.random.randint(1, 8, num_layers_conv).tolist()\n",
    "        pooling_kernel_size = np.random.randint(2, 4, num_layers_conv).tolist()\n",
    "        p = np.random.rand(num_layers_fc+num_layers_conv).tolist()\n",
    "        lambdda = parameters['lambda'][np.random.randint(1, len(parameters['lambda']))]\n",
    "        lr = parameters['lr'][np.random.randint(1, len(parameters['lr']))]\n",
    "        print(\"num_layers_fc: {}\\nnum_layers_conv: {}\\nlayers_fc: {}\\nlayers_conv: {}\\nkernel_size: {}\\n\\\n",
    "    pooling_kernel_size: {}\\np: {}\\nlambdda: {}\\nlr: {}\"\\\n",
    "              .format(num_layers_fc, num_layers_conv, layers_fc, layers_conv, kernel_size, \\\n",
    "                      pooling_kernel_size,p, lambdda, lr))\n",
    "        param = {'type': 'conv', 'layers': layers_fc, 'layers_conv':layers_conv, 'kernel_size': kernel_size, \\\n",
    "                  'pooling_kernel_size': pooling_kernel_size, 'p_list': p}\n",
    "        loss_train_kfold, loss_val_kfold, acc_train_kfold, acc_val_kfold = train_model_full(param, train_input, train_target, kfolds, nb_epochs, lambdda, lr)\n",
    "    except: pass \n",
    "    #Exception as err:\n",
    "     #   print(type(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature with time\n",
    "#steap and flat for right/left\n",
    "#throw away some features, i.e. feature 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
