{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import dlc_bci as bci\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target = bci.load(root = './data_bci')\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "X = train_input.numpy()\n",
    "y = train_target.numpy()\n",
    "kfolds = model_selection.KFold(n_splits=10, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is done!\n"
     ]
    }
   ],
   "source": [
    "# put this inside the train to avoid data snooping\n",
    "mu, std = train_input.mean(0), train_input.std(0)\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "print(\"Normalization is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input  = Variable(test_input)\n",
    "test_target = Variable(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_full(param, train_input, train_target, kfolds, nb_epochs, lambdda = 0.01, lr = 0.001):\n",
    "    \n",
    "    acc_train_kfold = []\n",
    "    loss_train_kfold = []\n",
    "    acc_val_kfold = []\n",
    "    loss_val_kfold = []\n",
    "    \n",
    "    for train_index, val_index in kfolds.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_train = Variable(torch.from_numpy(X_train))\n",
    "        X_val = Variable(torch.from_numpy(X_val))\n",
    "        y_train = Variable(torch.from_numpy(y_train))\n",
    "        y_val = Variable(torch.from_numpy(y_val)) \n",
    "        \n",
    "        if param['type']=='fc': model = FC_net(param['layers'])\n",
    "        elif param['type']=='conv': model = model_conv = Conv_net(param['layers'], param['layers_conv'], \\\n",
    "                                                    param['kernel_size'], param['pooling_kernel_size'], param['p_list'])\n",
    "        \n",
    "        loss_train, loss_val, acc_train, acc_val = train_model(model, X_train, y_train, X_val, y_val, kfolds, nb_epochs, lambdda, lr)\n",
    "        acc_train_kfold.append(acc_train)\n",
    "        loss_train_kfold.append(loss_train)\n",
    "        acc_val_kfold.append(acc_val)\n",
    "        loss_val_kfold.append(loss_val)\n",
    "        \n",
    "    acc_train_kfold = np.mean(np.array(acc_train_kfold), axis=0)\n",
    "    acc_val_kfold = np.mean(np.array(acc_val_kfold), axis=0)\n",
    "\n",
    "    loss_train_kfold = np.mean(np.array(loss_train_kfold), axis=0)\n",
    "    loss_val_kfold = np.mean(np.array(loss_val_kfold), axis=0)\n",
    "    \n",
    "    print('\\n\\n---- Epochs Done -----\\n')\n",
    "    print('Loss: train ~ {} Acc train ~ {} \\n   Loss: val ~ {} / Acc val ~ {}\\n'\n",
    "         .format(round(loss_train_kfold[-1], 3), \n",
    "                 round(acc_train_kfold[-1], 3), \n",
    "                 round(loss_val_kfold[-1], 3), \n",
    "                 round(acc_val_kfold[-1], 3)))\n",
    "    return loss_train_kfold, loss_val_kfold, acc_train_kfold, acc_val_kfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, kfolds, nb_epochs, lambdda = 0.01, lr = 0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "        \n",
    "    for e in range(0, nb_epochs):\n",
    "         \n",
    "        model.train(True)\n",
    "        for b in list(range(0, X_train.size(0), mini_batch_size)):\n",
    "            if b + mini_batch_size <= X_train.size(0):\n",
    "                output = model(X_train.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, y_train.narrow(0, b, mini_batch_size))\n",
    "            else:\n",
    "                output = model(X_train.narrow(0, b, X_train.size(0) - b))\n",
    "                loss = criterion(output, y_train.narrow(0, b, X_train.size(0) - b))\n",
    "\n",
    "            for p in model.parameters():\n",
    "                loss += lambdda*p.pow(2).sum()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            model.train(False)\n",
    "            output_train = model(X_train)\n",
    "            output_val = model(X_val)\n",
    "            \n",
    "        acc_val.append(1-compute_nb_errors(model, X_val, y_val, mini_batch_size=mini_batch_size)/X_val.size(0))\n",
    "        acc_train.append(1-compute_nb_errors(model, X_train, y_train, mini_batch_size=mini_batch_size)/X_train.size(0))\n",
    "        loss_train.append(criterion(output_train, y_train).data[0])\n",
    "        loss_val.append(criterion(output_val, y_val).data[0])\n",
    "        \n",
    "#         if (e % 100 == 0):\n",
    "#                 print('Epoch {}: \\n   CVLoss: train ~ {} CVAcc train ~ {} \\n   CVLoss: val ~ {} / CVAcc val ~ {}'\n",
    "#                   .format(e, \n",
    "#                           round(loss_train_epoch[-1], 3),\n",
    "#                           round(acc_train_epoch[-1], 3), \n",
    "#                           round(loss_val_epoch[-1], 3),\n",
    "#                           round(acc_val_epoch[-1], 3)))\n",
    "        \n",
    "#     print('\\n\\n---- Epochs Done -----\\n')\n",
    "#     print('CVLoss: train ~ {} CVAcc train ~ {} \\n   CVLoss: val ~ {} / CVAcc val ~ {}'\n",
    "#          .format(round(loss_train_epoch[-1], 3), \n",
    "#                  round(acc_train_epoch[-1], 3), \n",
    "#                  round(loss_val_epoch[-1], 3), \n",
    "#                  round(acc_val_epoch[-1], 3)))\n",
    "\n",
    "    return loss_train, loss_val, acc_train, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        if b + mini_batch_size <= data_input.size(0):\n",
    "            output = model(data_input.narrow(0, b, mini_batch_size))\n",
    "            _, predicted_classes = torch.max(output.data, 1)\n",
    "            for k in range(0, mini_batch_size):\n",
    "                if data_target.data[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "        else:       \n",
    "            output = model(data_input.narrow(0, b, data_input.size(0) - b))\n",
    "            _, predicted_classes = torch.max(output.data, 1)\n",
    "            for k in range(0, data_input.size(0) - b):\n",
    "                if data_target.data[b + k] != predicted_classes[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fully connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_net(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(FC_net, self).__init__() \n",
    "        self.additional_hidden = nn.ModuleList()\n",
    "        for l in range(len(layers)-1):\n",
    "            self.additional_hidden.append(nn.Linear(layers[l], layers[l+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(x.shape[0], -1)\n",
    "        for l in range(len(self.additional_hidden)-1):\n",
    "            x = F.relu(self.additional_hidden[l](x))\n",
    "        x = self.additional_hidden[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check model parameters\n",
    "layers = [train_input.view(train_input.shape[0], -1).shape[1], 5, 5, 2]\n",
    "# for k in model_fc.parameters():\n",
    "#     print(k.size())\n",
    "parameters = {'type': 'fc', 'layers': layers}\n",
    "    \n",
    "mini_batch_size = 42\n",
    "nb_epochs = 10\n",
    "#costs, costs_val, acc, acc_val = train_model_full(parameters, train_input, train_target, kfolds, nb_epochs, lambdda=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "# ax1.plot (range(nb_epochs), costs)\n",
    "# ax1.plot (range(nb_epochs), costs_val)\n",
    "# ax2.plot (range(nb_epochs), acc)\n",
    "# ax2.plot (range(nb_epochs), acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('train_error {:.02f}%'.format(\n",
    "#            compute_nb_errors(model_fc, Variable(train_input), Variable(train_target), mini_batch_size = 79) / train_input.size(0) * 100))\n",
    "#print('test_error {:.02f}%'.format(\n",
    "#            compute_nb_errors(model_fc, test_input, test_target, mini_batch_size = 20) / test_input.size(0) * 100))\n",
    "\n",
    "# print(\"train data error = {}/316 %\".format(compute_nb_errors(model, train_input, train_target))\n",
    "# compute_nb_errors(model, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model few times and take average, because of different initialization\n",
    "#figure out the case when is 50% error for both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_net(nn.Module):\n",
    "    def __init__(self, layers, layers_conv, kernel_size, pooling_kernel_size, p):\n",
    "        super(Conv_net, self).__init__()\n",
    "        self.pooling_kernel_size = pooling_kernel_size\n",
    "        self.additional_conv_hidden = nn.ModuleList()\n",
    "        self.additional_fc_hidden = nn.ModuleList()\n",
    "        self.droput_layers = nn.ModuleList()\n",
    "        self.batch_normalization = nn.ModuleList()\n",
    "        \n",
    "        for l in range(len(layers_conv)-1):\n",
    "            self.additional_conv_hidden.append(nn.Conv1d(layers_conv[l], layers_conv[l+1], kernel_size=kernel_size[l]))\n",
    "            self.droput_layers.append(torch.nn.Dropout(p=p[l]))\n",
    "            self.batch_normalization.append(torch.nn.BatchNorm1d(layers_conv[l+1]))\n",
    "        size = train_input.shape[2]\n",
    "\n",
    "        for i in range(len(kernel_size)):\n",
    "            size-=(kernel_size[i]-1)\n",
    "\n",
    "            size//=pooling_kernel_size[i]\n",
    "\n",
    "        self.additional_fc_hidden.append(nn.Linear(size*layers_conv[-1], layers[0]))\n",
    "        self.droput_layers.append(torch.nn.Dropout(p=p[l+1]))\n",
    "        self.batch_normalization.append(torch.nn.BatchNorm1d(layers[0]))\n",
    "        self.flat_size = size*layers_conv[-1]\n",
    "        \n",
    "        start_p = l+2\n",
    "\n",
    "        for l in range(len(layers)-1):\n",
    "            self.additional_fc_hidden.append(nn.Linear(layers[l], layers[l+1]))\n",
    "            if l != len(layers)-2:\n",
    "                self.droput_layers.append(torch.nn.Dropout(p=p[l+start_p]))\n",
    "                self.batch_normalization.append(torch.nn.BatchNorm1d(layers[l+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in range(len(self.additional_conv_hidden)):\n",
    "            x = self.droput_layers[l](self.batch_normalization[l](F.relu(F.max_pool1d(self.additional_conv_hidden[l](x), \\\n",
    "                                                          kernel_size=self.pooling_kernel_size[l]))))\n",
    "        x=x.view(-1, self.flat_size)\n",
    "        for l in range(len(self.additional_fc_hidden)-1):\n",
    "            index = len(self.additional_conv_hidden)+l\n",
    "            x = self.droput_layers[index](self.batch_normalization[index](F.relu(self.additional_fc_hidden[l](x))))\n",
    "        x = self.additional_fc_hidden[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [0.2, 0.2, 0]\n",
    "layers = [5, 2]\n",
    "layers_conv = [28, 4, 4]\n",
    "kernel_size = [6, 6]\n",
    "pooling_kernel_size = [3, 2]\n",
    "# for k in model_conv.parameters():\n",
    "#     print(k.size())\n",
    "    \n",
    "parameters = {'type': 'conv', 'layers': layers, 'layers_conv':layers_conv, 'kernel_size': kernel_size, \\\n",
    "              'pooling_kernel_size': pooling_kernel_size, 'p_list': p_list}\n",
    "    \n",
    "mini_batch_size = 79\n",
    "nb_epochs = 500\n",
    "#costs, costs_val, acc, acc_val = train_model_full(parameters, train_input, train_target, kfolds, nb_epochs, lambdda=0.0375)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "# fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "# ax1.plot (range(nb_epochs), costs)\n",
    "# ax1.plot (range(nb_epochs), costs_val)\n",
    "# ax2.plot (range(nb_epochs), acc)\n",
    "# ax2.plot (range(nb_epochs), acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('train_error {:.02f}%'.format(\n",
    "#             compute_nb_errors(model_conv, Variable(train_input), Variable(train_target), mini_batch_size = 79) / train_input.size(0) * 100))\n",
    "# print('test_error {:.02f}%'.format(\n",
    "#             compute_nb_errors(model_conv, test_input, test_target, mini_batch_size = 20) / test_input.size(0) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \n",
    "     'lambda': np.linspace(0.01, 0.1, 30).tolist()+[0], \n",
    "     'lr': np.linspace(0.001, 0.1, 50).tolist()\n",
    "}\n",
    "nb_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parallel(param):\n",
    "    train_model_full(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [37, 11, 49, 2]\n",
      "layers_conv: [28, 16, 4, 5, 8]\n",
      "kernel_size: [7, 7, 2, 7]\n",
      "    pooling_kernel_size: [2, 2, 2, 3]\n",
      "p: [0.12612464032468163, 0.9771354261124524, 0.5252784704386979, 0.9271068719568062, 0.4686411047290404, 0.19720970046520792, 0.8220800779956581]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.031306122448979595\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [7, 23, 13, 2]\n",
      "layers_conv: [28, 14, 15, 19, 3]\n",
      "kernel_size: [1, 7, 7, 3]\n",
      "    pooling_kernel_size: [2, 2, 3, 3]\n",
      "p: [0.08517354911809227, 0.9592063663657461, 0.8329841800949869, 0.936062954817736, 0.48192119230104213, 0.9616798386796582, 0.8271796127358846]\n",
      "lambdda: 0.1\n",
      "lr: 0.08383673469387756\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 9\n",
      "layers_fc: [21, 2, 2]\n",
      "layers_conv: [28, 3, 13, 5, 13, 5, 10, 1, 17, 11]\n",
      "kernel_size: [4, 4, 5, 4, 3, 4, 1, 4, 6]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 3, 3, 3, 3, 3]\n",
      "p: [0.9444132706665297, 0.8522489709841847, 0.7475197077852769, 0.6349341466330399, 0.4647222267428406, 0.96279564617646, 0.14804134095969057, 0.9706584156249445, 0.3301536904118194, 0.7496104202799089, 0.799685466939821]\n",
      "lambdda: 0.04413793103448276\n",
      "lr: 0.03736734693877551\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 9\n",
      "layers_fc: [25, 2]\n",
      "layers_conv: [28, 10, 2, 5, 6, 1, 8, 4, 1, 9]\n",
      "kernel_size: [3, 4, 7, 5, 4, 7, 6, 7, 2]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 2, 2, 2, 2]\n",
      "p: [0.8097096978211238, 0.9343714600406676, 0.6153897301827846, 0.5548324456968505, 0.9133063537101359, 0.5658863404271331, 0.4494805441781544, 0.7048259189636696, 0.9611237457413896, 0.8865273335594585]\n",
      "lambdda: 0.0906896551724138\n",
      "lr: 0.0333265306122449\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [9, 47, 22, 2]\n",
      "layers_conv: [28, 10, 7, 15, 15]\n",
      "kernel_size: [7, 7, 6, 7]\n",
      "    pooling_kernel_size: [3, 3, 2, 3]\n",
      "p: [0.7516865288751294, 0.5666377154634482, 0.07820935315679844, 0.9635640387091815, 0.01981536761122238, 0.670849270014542, 0.7504280006847014]\n",
      "lambdda: 0.09379310344827586\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [19, 7, 12, 2]\n",
      "layers_conv: [28, 9, 5, 12, 19, 18, 19, 18]\n",
      "kernel_size: [4, 2, 6, 2, 1, 4, 4]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3, 2, 3]\n",
      "p: [0.9555350890039488, 0.8461980289981703, 0.725149817074867, 0.9006885105405454, 0.2029841084228433, 0.1338968251541527, 0.14753082939628348, 0.9390876038857687, 0.8673461738854705, 0.5078293526143273]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.0656530612244898\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [9, 46, 6, 2]\n",
      "layers_conv: [28, 7, 3, 19]\n",
      "kernel_size: [5, 3, 4]\n",
      "    pooling_kernel_size: [3, 2, 2]\n",
      "p: [0.2184701198375839, 0.7943177277689486, 0.44555708609099864, 0.8633739132972612, 0.39583423484516134, 0.7306388045259881]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.05757142857142858\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [45, 11, 7, 2]\n",
      "layers_conv: [28, 9, 3, 3, 5, 4, 3, 10]\n",
      "kernel_size: [6, 4, 7, 6, 7, 6, 4]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 2, 3]\n",
      "p: [0.5827751980696244, 0.4033033133540068, 0.5708453084288708, 0.3398629694331604, 0.17687352048994276, 0.8582390484375164, 0.31269444999945395, 0.6422277765548627, 0.45362183541631473, 0.8558722526709801]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.07171428571428572\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 8\n",
      "layers_fc: [16, 2]\n",
      "layers_conv: [28, 7, 6, 10, 17, 6, 15, 1, 13]\n",
      "kernel_size: [3, 1, 5, 1, 4, 7, 5, 7]\n",
      "    pooling_kernel_size: [2, 2, 2, 3, 3, 2, 2, 3]\n",
      "p: [0.5048742023648325, 0.25181438712755977, 0.787916673728576, 0.9003411166276518, 0.3459291129905965, 0.2632680992687001, 0.4447760013230895, 0.9903684752210583, 0.577269507105748]\n",
      "lambdda: 0.08758620689655172\n",
      "lr: 0.0333265306122449\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [16, 24, 2]\n",
      "layers_conv: [28, 5, 18, 11, 17, 17, 19, 13]\n",
      "kernel_size: [6, 2, 7, 3, 5, 7, 2]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 2, 2, 3]\n",
      "p: [0.8089301211029175, 0.9055064405165347, 0.6362797937638868, 0.7884372873191274, 0.5948784225770603, 0.702264193914445, 0.7564838254892994, 0.6749735326934361, 0.1760148301878166]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.04544897959183674\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [23, 2]\n",
      "layers_conv: [28, 9, 3, 14]\n",
      "kernel_size: [2, 2, 3]\n",
      "    pooling_kernel_size: [2, 3, 3]\n",
      "p: [0.38839951350098467, 0.7800864274156151, 0.3266612624599078, 0.1709662043650344]\n",
      "lambdda: 0.0906896551724138\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [42, 35, 1, 2]\n",
      "layers_conv: [28, 10, 11, 2, 16, 3, 15]\n",
      "kernel_size: [3, 4, 4, 5, 2, 4]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 2, 2]\n",
      "p: [0.37531250511084757, 0.28176208583122186, 0.05988789795472704, 0.7926388611785611, 0.9658270427206522, 0.46211804666219325, 0.6094751937741876, 0.16795238766706544, 0.7721214470668956]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.08383673469387756\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [13, 26, 2]\n",
      "layers_conv: [28, 13, 18, 19]\n",
      "kernel_size: [5, 2, 1]\n",
      "    pooling_kernel_size: [3, 2, 3]\n",
      "p: [0.8349363315209353, 0.7448932364082921, 0.8616595616422238, 0.27315734934409386, 0.7766639242230986]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 2\n",
      "layers_fc: [28, 2]\n",
      "layers_conv: [28, 2, 13]\n",
      "kernel_size: [1, 6]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.8316606901719159, 0.08649793437571784, 0.6671496661774498]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.08181632653061224\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [36, 2]\n",
      "layers_conv: [28, 7]\n",
      "kernel_size: [5]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.27659306643567516, 0.9534860675284513]\n",
      "lambdda: 0.0906896551724138\n",
      "lr: 0.07777551020408163\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [34, 2]\n",
      "layers_conv: [28, 16, 16, 19, 6, 4, 9]\n",
      "kernel_size: [1, 5, 2, 5, 4, 2]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 3, 2]\n",
      "p: [0.4560749005521648, 0.2419680995683181, 0.8747775953806188, 0.22270414440560082, 0.2247778775737146, 0.08957787485373747, 0.28227009517015067]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.07373469387755102\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [9, 2]\n",
      "layers_conv: [28, 2, 19, 1]\n",
      "kernel_size: [4, 7, 5]\n",
      "    pooling_kernel_size: [3, 2, 2]\n",
      "p: [0.731801594659297, 0.8692655805939693, 0.3013330828529298, 0.47831352729222987]\n",
      "lambdda: 0.07206896551724139\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [23, 33, 12, 2]\n",
      "layers_conv: [28, 2, 12, 10, 9]\n",
      "kernel_size: [7, 5, 2, 7]\n",
      "    pooling_kernel_size: [2, 2, 3, 2]\n",
      "p: [0.3660652792340575, 0.4294061548114112, 0.744273632294853, 0.9123745019956525, 0.8113146856677798, 0.08797892538644392, 0.08257748316880409]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.015142857142857145\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [1, 26, 33, 2]\n",
      "layers_conv: [28, 10, 11, 19]\n",
      "kernel_size: [5, 2, 4]\n",
      "    pooling_kernel_size: [3, 2, 3]\n",
      "p: [0.33525956234768883, 0.04170967490124744, 0.05582522366553189, 0.38730828386711325, 0.8519343429899758, 0.6537024596526887]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [33, 2]\n",
      "layers_conv: [28, 13, 11, 6]\n",
      "kernel_size: [2, 7, 3]\n",
      "    pooling_kernel_size: [3, 3, 2]\n",
      "p: [0.8916658395166147, 0.9536618786198262, 0.002945613780047762, 0.7699572647468266]\n",
      "lambdda: 0.08758620689655172\n",
      "lr: 0.06161224489795919\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 4\n",
      "layers_fc: [46, 19, 2]\n",
      "layers_conv: [28, 7, 19, 18, 2]\n",
      "kernel_size: [1, 2, 4, 2]\n",
      "    pooling_kernel_size: [3, 3, 2, 2]\n",
      "p: [0.20394155952272564, 0.7190408918302795, 0.18784709282793843, 0.8745397512115025, 0.270972013850694, 0.7052225168638089]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.04140816326530612\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [11, 10, 31, 2]\n",
      "layers_conv: [28, 3, 9, 7, 1, 4, 18, 4, 8, 16]\n",
      "kernel_size: [2, 7, 6, 4, 2, 4, 6, 4, 7]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 3, 3, 3, 3, 3]\n",
      "p: [0.13962901852313903, 0.11148066390749756, 0.8846460837257263, 0.6647874346988569, 0.4875954438735388, 0.5086667689323549, 0.8914620314946976, 0.7715877370322705, 0.281706185705091, 0.4958694830386182, 0.9096299045717472, 0.838509231711672]\n",
      "lambdda: 0.019310344827586208\n",
      "lr: 0.08989795918367348\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [29, 2]\n",
      "layers_conv: [28, 15, 1, 12, 16, 4]\n",
      "kernel_size: [2, 7, 1, 5, 5]\n",
      "    pooling_kernel_size: [2, 2, 3, 2, 3]\n",
      "p: [0.5657894102002662, 0.7297294277845172, 0.7364946278496011, 0.33011892816548727, 0.9624079738361161, 0.023538364213626095]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [49, 48, 2]\n",
      "layers_conv: [28, 4, 3, 1, 17, 15]\n",
      "kernel_size: [3, 3, 4, 3, 5]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 3]\n",
      "p: [0.3955122159417328, 0.4254086780694061, 0.865129130995096, 0.8273789815892226, 0.29759467131612705, 0.06478514401408964, 0.6871567881845051]\n",
      "lambdda: 0.025517241379310347\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [30, 35, 2]\n",
      "layers_conv: [28, 1, 10, 2, 13, 17, 13]\n",
      "kernel_size: [7, 6, 5, 2, 3, 3]\n",
      "    pooling_kernel_size: [3, 2, 2, 2, 3, 3]\n",
      "p: [0.220230413915808, 0.0914682278127692, 0.07600902288132683, 0.19629706963981097, 0.624819634566905, 0.0884315352774313, 0.45277325466512275, 0.41341040476003976]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [18, 19, 2]\n",
      "layers_conv: [28, 2]\n",
      "kernel_size: [7]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.9512054164939011, 0.5128898593794408, 0.6870481113418633]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 8\n",
      "layers_fc: [14, 21, 2]\n",
      "layers_conv: [28, 19, 10, 12, 17, 7, 12, 14, 6]\n",
      "kernel_size: [2, 6, 5, 6, 3, 5, 7, 2]\n",
      "    pooling_kernel_size: [2, 2, 3, 2, 2, 2, 3, 3]\n",
      "p: [0.14117165881350535, 0.20373033048255362, 0.41228237094607323, 0.37681482380274645, 0.9730306631836383, 0.12786576534689342, 0.5504498714478298, 0.5741521588585913, 0.6934307844771849, 0.8995217554547777]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.07373469387755102\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [23, 2]\n",
      "layers_conv: [28, 9, 3, 7]\n",
      "kernel_size: [6, 5, 2]\n",
      "    pooling_kernel_size: [3, 3, 2]\n",
      "p: [0.20715444972973052, 0.42593000006512005, 0.9684712562325, 0.3757098744095607]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.01716326530612245\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [33, 32, 2]\n",
      "layers_conv: [28, 12, 17, 4, 14, 4]\n",
      "kernel_size: [6, 2, 7, 7, 2]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 3]\n",
      "p: [0.914547263473453, 0.19065001444451957, 0.6577588945488242, 0.870650005208559, 0.439473843650924, 0.3539477156524721, 0.9668670921809648]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.031306122448979595\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [7, 2]\n",
      "layers_conv: [28, 17, 19, 2]\n",
      "kernel_size: [7, 4, 4]\n",
      "    pooling_kernel_size: [2, 3, 3]\n",
      "p: [0.7770741940484888, 0.9986514896429715, 0.3371253524630814, 0.5130176587128003]\n",
      "lambdda: 0.0906896551724138\n",
      "lr: 0.0636326530612245\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 7\n",
      "layers_fc: [32, 2]\n",
      "layers_conv: [28, 16, 6, 15, 13, 7, 7, 4]\n",
      "kernel_size: [3, 3, 6, 2, 6, 6, 6]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 2, 3, 3]\n",
      "p: [0.3538815717457391, 0.522177623431124, 0.10860531249065752, 0.8784946387005794, 0.8329977570858823, 0.011995387164251192, 0.3726095456244709, 0.4835036720416155]\n",
      "lambdda: 0.01310344827586207\n",
      "lr: 0.06161224489795919\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 8\n",
      "layers_fc: [45, 32, 2]\n",
      "layers_conv: [28, 13, 18, 5, 17, 7, 15, 17, 7]\n",
      "kernel_size: [1, 5, 5, 7, 7, 3, 1, 6]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 3, 2, 2, 3]\n",
      "p: [0.44661999886589154, 0.4254394892534338, 0.7394202740571445, 0.8848456746544148, 0.5444643474980324, 0.25511111989004287, 0.6263905103141854, 0.3151283478001937, 0.9116538750619512, 0.43377405729060026]\n",
      "lambdda: 0.1\n",
      "lr: 0.07777551020408163\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [41, 23, 2]\n",
      "layers_conv: [28, 10, 12, 3]\n",
      "kernel_size: [3, 7, 1]\n",
      "    pooling_kernel_size: [3, 2, 2]\n",
      "p: [0.24475447696985142, 0.925246936175218, 0.07269101743720263, 0.5654094967508225, 0.8792135419589387]\n",
      "lambdda: 0.0906896551724138\n",
      "lr: 0.047469387755102045\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [3, 47, 2]\n",
      "layers_conv: [28, 10, 17, 17, 9, 9]\n",
      "kernel_size: [1, 6, 5, 6, 4]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 3]\n",
      "p: [0.5331024099049857, 0.6595765812616656, 0.6525510467151733, 0.6471275237365137, 0.29594043054853325, 0.19366329300625285, 0.5360062652911494]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.0636326530612245\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [46, 30, 2]\n",
      "layers_conv: [28, 7, 15]\n",
      "kernel_size: [6, 4]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.2632623635920154, 0.779002582685672, 0.17906778187701755, 0.3244781565562964]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.013122448979591837\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [10, 30, 42, 2]\n",
      "layers_conv: [28, 9, 15, 17, 6, 6, 11, 18]\n",
      "kernel_size: [3, 2, 6, 5, 7, 1, 6]\n",
      "    pooling_kernel_size: [3, 2, 3, 2, 3, 3, 2]\n",
      "p: [0.5906651934286974, 0.3642331915626744, 0.4576479062763058, 0.08085164102004583, 0.17456546764993575, 0.9805555608161138, 0.5381586444872796, 0.41667402805826126, 0.3517603780190469, 0.05493015722989447]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [15, 6, 45, 2]\n",
      "layers_conv: [28, 18, 10, 1, 10, 8, 17, 9, 3, 6]\n",
      "kernel_size: [3, 2, 5, 1, 6, 6, 6, 3, 2]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 2, 2, 2, 2, 2]\n",
      "p: [0.6546504373204101, 0.045168568852540836, 0.5514111368545621, 0.8628100399589969, 0.2828324496297636, 0.9585358347603292, 0.794802828016958, 0.21286682503750198, 0.8100665989984172, 0.8771436343299325, 0.18998631976833202, 0.47350774305705645]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.04948979591836735\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [32, 5, 24, 2]\n",
      "layers_conv: [28, 13, 5, 13]\n",
      "kernel_size: [2, 1, 2]\n",
      "    pooling_kernel_size: [2, 2, 2]\n",
      "p: [0.05187084419342847, 0.6609570492336627, 0.06045288829766993, 0.9164889693294963, 0.7946107583390013, 0.6550158955544169]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.011102040816326531\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 3\n",
      "layers_fc: [41, 35, 32, 2]\n",
      "layers_conv: [28, 11, 7, 9]\n",
      "kernel_size: [4, 7, 5]\n",
      "    pooling_kernel_size: [2, 3, 2]\n",
      "p: [0.6988864410998997, 0.13107237972125574, 0.46567208211329525, 0.044812214715032495, 0.7540194406126804, 0.29440142721303486]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.09191836734693878\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [5, 6, 2]\n",
      "layers_conv: [28, 14, 13, 1]\n",
      "kernel_size: [5, 2, 2]\n",
      "    pooling_kernel_size: [3, 2, 2]\n",
      "p: [0.8247188970428927, 0.7416953035365267, 0.7961157119637722, 0.7626373455036699, 0.9516816700071999]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.0333265306122449\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [26, 47, 14, 2]\n",
      "layers_conv: [28, 9, 19, 11, 17, 19]\n",
      "kernel_size: [4, 4, 2, 1, 7]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 3]\n",
      "p: [0.10696594911981439, 0.09474470813412372, 0.72772232742125, 0.38122345856074724, 0.3246916254961336, 0.556098819628246, 0.47442025507193986, 0.03542019573393096]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 9\n",
      "layers_fc: [13, 26, 2]\n",
      "layers_conv: [28, 14, 9, 7, 16, 4, 7, 15, 5, 7]\n",
      "kernel_size: [4, 1, 7, 3, 6, 7, 7, 6, 1]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 3, 3, 3, 2]\n",
      "p: [0.35292801606931534, 0.42390647313429886, 0.16179268330450125, 0.5975910721645933, 0.924609331550686, 0.13814460799237538, 0.2776747289059064, 0.04990326985145754, 0.6233295217694286, 0.509463412852644, 0.14026576716984773]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.07777551020408163\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [32, 23, 38, 2]\n",
      "layers_conv: [28, 15, 18, 3, 15]\n",
      "kernel_size: [6, 4, 4, 1]\n",
      "    pooling_kernel_size: [3, 2, 2, 3]\n",
      "p: [0.3995963583078491, 0.6753135641617343, 0.12267527212493046, 0.8112197852724896, 0.7749243891714044, 0.05162466573072877, 0.8204323954897399]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.0979795918367347\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 7\n",
      "layers_fc: [12, 33, 27, 2]\n",
      "layers_conv: [28, 8, 8, 18, 12, 18, 16, 15]\n",
      "kernel_size: [1, 1, 6, 6, 5, 6, 7]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 3, 3, 3]\n",
      "p: [0.10355623193795749, 0.597185430753489, 0.15476533833083295, 0.09681012628557872, 0.3013630632724533, 0.3199977019398418, 0.5731903257750087, 0.529692502953881, 0.8646629880177698, 0.23696795690181027]\n",
      "lambdda: 0.08448275862068966\n",
      "lr: 0.08383673469387756\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [10, 28, 2]\n",
      "layers_conv: [28, 11, 16, 14, 8, 11, 2, 10]\n",
      "kernel_size: [6, 4, 3, 2, 7, 4, 7]\n",
      "    pooling_kernel_size: [2, 3, 2, 3, 2, 3, 2]\n",
      "p: [0.0038333106336935208, 0.8269483507362325, 0.5326096540414049, 0.3395673522379703, 0.3201575768786048, 0.5880215749194377, 0.715653716645176, 0.8903511155214827, 0.7708511632216777]\n",
      "lambdda: 0.031724137931034485\n",
      "lr: 0.013122448979591837\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 8\n",
      "layers_fc: [31, 24, 2]\n",
      "layers_conv: [28, 18, 3, 1, 8, 3, 13, 13, 1]\n",
      "kernel_size: [2, 6, 7, 6, 2, 1, 2, 7]\n",
      "    pooling_kernel_size: [2, 2, 2, 3, 2, 2, 2, 2]\n",
      "p: [0.4490981267369937, 0.5663407611183616, 0.9334254585285751, 0.28173927438476065, 0.8089886700203417, 0.05663035658726967, 0.4595757306404106, 0.5327768938694177, 0.9719040630705085, 0.7058628988061266]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 9\n",
      "layers_fc: [25, 2]\n",
      "layers_conv: [28, 16, 18, 7, 11, 15, 17, 4, 8, 10]\n",
      "kernel_size: [7, 3, 2, 5, 7, 5, 6, 7, 7]\n",
      "    pooling_kernel_size: [3, 2, 2, 3, 2, 2, 3, 3, 2]\n",
      "p: [0.33784590403698067, 0.2617531314985725, 0.7098720942907372, 0.43570811420544453, 0.8786878395329872, 0.8008070688903535, 0.3423132471009104, 0.9503038191524147, 0.3965096284649885, 0.5549509172173515]\n",
      "lambdda: 0.025517241379310347\n",
      "lr: 0.013122448979591837\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [46, 2]\n",
      "layers_conv: [28, 11, 15, 16, 10, 8]\n",
      "kernel_size: [1, 5, 4, 1, 1]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2]\n",
      "p: [0.5775887439789472, 0.8477465950208704, 0.5882977057802411, 0.7976658763721131, 0.7479152020968479, 0.49992100450302035]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.0959591836734694\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [49, 42, 2]\n",
      "layers_conv: [28, 13]\n",
      "kernel_size: [1]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.6226166785128671, 0.10202068878456816, 0.6777695462327189]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.08989795918367348\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 9\n",
      "layers_fc: [35, 2]\n",
      "layers_conv: [28, 19, 16, 14, 13, 14, 8, 19, 19, 7]\n",
      "kernel_size: [2, 1, 2, 1, 7, 6, 7, 1, 5]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 2, 3, 2, 2, 2]\n",
      "p: [0.5260175903093911, 0.034492084329416084, 0.6870666743961223, 0.6680982647167258, 0.4659404632137564, 0.0952164076200589, 0.7679152261601148, 0.6018445267038723, 0.1415428415703035, 0.486045663993821]\n",
      "lambdda: 0.04724137931034483\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [48, 48, 23, 2]\n",
      "layers_conv: [28, 13, 6, 13, 9]\n",
      "kernel_size: [5, 4, 3, 2]\n",
      "    pooling_kernel_size: [3, 3, 3, 2]\n",
      "p: [0.3491921861894045, 0.951624499468269, 0.18492039548874406, 0.03550028220023127, 0.0939356616098348, 0.58147642877644, 0.8455652726936638]\n",
      "lambdda: 0.09379310344827586\n",
      "lr: 0.05757142857142858\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [39, 13, 2]\n",
      "layers_conv: [28, 7, 10, 17, 18, 5]\n",
      "kernel_size: [2, 4, 1, 6, 2]\n",
      "    pooling_kernel_size: [2, 2, 2, 3, 2]\n",
      "p: [0.162865572360127, 0.23634768762570602, 0.14409072573584503, 0.2370397672876956, 0.9160780878807083, 0.8186911163021081, 0.6980140584331561]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.035346938775510206\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 2\n",
      "layers_fc: [41, 2]\n",
      "layers_conv: [28, 5, 3]\n",
      "kernel_size: [5, 6]\n",
      "    pooling_kernel_size: [2, 2]\n",
      "p: [0.08547009415211215, 0.20391784782917755, 0.9753872904900478]\n",
      "lambdda: 0.08448275862068966\n",
      "lr: 0.08989795918367348\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [9, 4, 2]\n",
      "layers_conv: [28, 1, 9, 14, 19, 8, 17]\n",
      "kernel_size: [1, 5, 7, 7, 1, 7]\n",
      "    pooling_kernel_size: [3, 3, 2, 2, 3, 3]\n",
      "p: [0.6377113669384977, 0.5163707931823301, 0.9635591411059833, 0.6220700354820711, 0.22400784507341587, 0.27449002732920236, 0.8871308113336692, 0.47743495670054725]\n",
      "lambdda: 0.09379310344827586\n",
      "lr: 0.04544897959183674\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [4, 26, 2]\n",
      "layers_conv: [28, 1, 1, 9, 16, 17, 17, 11]\n",
      "kernel_size: [1, 4, 5, 6, 5, 1, 4]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3, 3, 2]\n",
      "p: [0.7764363277948442, 0.4199162336433212, 0.75177859065124, 0.544667043930818, 0.6685847069665388, 0.5165994918908398, 0.42371498739860003, 0.48681640397341674, 0.7544979710064871]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.08383673469387756\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [44, 33, 2]\n",
      "layers_conv: [28, 14, 19, 13, 2, 5]\n",
      "kernel_size: [1, 2, 7, 4, 2]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 3]\n",
      "p: [0.19463319792391665, 0.8435180837355786, 0.6922370523912037, 0.6205761251193358, 0.7192814633209664, 0.4304391095804275, 0.11113775168045947]\n",
      "lambdda: 0.05965517241379311\n",
      "lr: 0.011102040816326531\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [3, 2]\n",
      "layers_conv: [28, 12]\n",
      "kernel_size: [6]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.7510035447331177, 0.2667681145950618]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.07979591836734694\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [44, 6, 24, 2]\n",
      "layers_conv: [28, 2, 9, 11, 1, 16, 17]\n",
      "kernel_size: [7, 6, 6, 3, 2, 7]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 2, 2]\n",
      "p: [0.9822148657527587, 0.6575406555052362, 0.25149684673011174, 0.35781651877379406, 0.10681380566108056, 0.31278218540629465, 0.9669025051646946, 0.4300382068635784, 0.7237660952179552]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.04948979591836735\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 8\n",
      "layers_fc: [13, 33, 5, 2]\n",
      "layers_conv: [28, 15, 10, 14, 12, 18, 6, 5, 19]\n",
      "kernel_size: [2, 3, 4, 6, 7, 6, 3, 6]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 2, 2, 3, 2]\n",
      "p: [0.33130304880321804, 0.7513414788330041, 0.12408406957537954, 0.08779676455818264, 0.5830004960990351, 0.34800415661319917, 0.971886021403794, 0.014886352983877305, 0.3717812230090509, 0.13635541741765866, 0.8899311509206919]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.07373469387755102\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 1\n",
      "layers_fc: [15, 2]\n",
      "layers_conv: [28, 5]\n",
      "kernel_size: [3]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.4583250020896702, 0.7001993638827313]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.051510204081632656\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [9, 2]\n",
      "layers_conv: [28, 17, 17, 9, 18, 9, 16]\n",
      "kernel_size: [7, 4, 4, 3, 6, 1]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 2, 3]\n",
      "p: [0.2549122387700872, 0.9288056483740338, 0.08848021228366776, 0.01000209847692124, 0.029931585373374592, 0.9596578292231971, 0.7352601937521572]\n",
      "lambdda: 0.07827586206896552\n",
      "lr: 0.07777551020408163\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 4\n",
      "layers_fc: [11, 2]\n",
      "layers_conv: [28, 1, 17, 10, 14]\n",
      "kernel_size: [5, 5, 7, 3]\n",
      "    pooling_kernel_size: [3, 2, 3, 3]\n",
      "p: [0.0653695044567334, 0.7969416178970915, 0.004512438497303428, 0.11985474507591543, 0.8845182960470558]\n",
      "lambdda: 0.1\n",
      "lr: 0.0050408163265306125\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [27, 7, 2]\n",
      "layers_conv: [28, 12, 7, 8, 12, 16, 5, 18]\n",
      "kernel_size: [2, 4, 2, 6, 6, 7, 5]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 2, 3, 2]\n",
      "p: [0.24763938464062352, 0.42837246553718067, 0.3261913072581831, 0.42379650047129336, 0.5935192909351286, 0.8441422648601496, 0.4753963414711856, 0.34616649473763883, 0.12577776916644212]\n",
      "lambdda: 0.031724137931034485\n",
      "lr: 0.06161224489795919\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 9\n",
      "layers_fc: [39, 2]\n",
      "layers_conv: [28, 12, 16, 16, 18, 3, 11, 5, 16, 3]\n",
      "kernel_size: [7, 1, 6, 7, 2, 2, 1, 1, 3]\n",
      "    pooling_kernel_size: [3, 2, 3, 3, 2, 3, 3, 2, 2]\n",
      "p: [0.2850577608926961, 0.8197752158162263, 0.5195452359089529, 0.47155421042291024, 0.941786103455044, 0.04040220926573013, 0.8279204723448167, 0.2916042269975757, 0.6094397791804459, 0.42551972954492534]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [49, 2]\n",
      "layers_conv: [28, 2, 9, 9, 14, 4]\n",
      "kernel_size: [5, 4, 5, 3, 2]\n",
      "    pooling_kernel_size: [2, 3, 3, 2, 2]\n",
      "p: [0.36565681446730003, 0.18992500676391977, 0.9186643606799388, 0.6275596464810798, 0.3070261170765599, 0.34719677439524665]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [6, 2]\n",
      "layers_conv: [28, 17, 9, 8]\n",
      "kernel_size: [1, 2, 4]\n",
      "    pooling_kernel_size: [2, 2, 2]\n",
      "p: [0.9596103741107459, 0.1450425530016608, 0.8612323741287671, 0.5956628500050519]\n",
      "lambdda: 0.0813793103448276\n",
      "lr: 0.047469387755102045\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [44, 16, 15, 2]\n",
      "layers_conv: [28, 1, 5, 1, 15, 1, 2]\n",
      "kernel_size: [4, 5, 5, 5, 3, 7]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 2, 2]\n",
      "p: [0.47008684951101176, 0.8484227430611125, 0.70979010611417, 0.49356314048610583, 0.9527757618229605, 0.35446102407063795, 0.8073800825231556, 0.2855636366930292, 0.8847118330915833]\n",
      "lambdda: 0.06275862068965518\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 3\n",
      "layers_fc: [31, 2]\n",
      "layers_conv: [28, 5, 14, 9]\n",
      "kernel_size: [1, 3, 7]\n",
      "    pooling_kernel_size: [2, 2, 2]\n",
      "p: [0.2850649132005456, 0.6063800343704516, 0.9601937920326306, 0.006721635440495088]\n",
      "lambdda: 0.0813793103448276\n",
      "lr: 0.04140816326530612\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [8, 36, 2]\n",
      "layers_conv: [28, 5]\n",
      "kernel_size: [5]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.17533196390250494, 0.2641220417490566, 0.6252649017962147]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.07575510204081633\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 1\n",
      "layers_fc: [38, 6, 16, 2]\n",
      "layers_conv: [28, 7]\n",
      "kernel_size: [4]\n",
      "    pooling_kernel_size: [2]\n",
      "p: [0.07278673215489417, 0.6356507804545477, 0.7215664341907898, 0.9662395692748168]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.015142857142857145\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 7\n",
      "layers_fc: [7, 2]\n",
      "layers_conv: [28, 3, 19, 1, 17, 10, 2, 8]\n",
      "kernel_size: [5, 1, 3, 1, 5, 2, 3]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 3, 2, 2]\n",
      "p: [0.9554327213164036, 0.8147950612235189, 0.12132273912574898, 0.5741029279879449, 0.9587152938224317, 0.4279948743482226, 0.690177545146558, 0.8032426658906966]\n",
      "lambdda: 0.01310344827586207\n",
      "lr: 0.043428571428571434\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [41, 24, 2]\n",
      "layers_conv: [28, 13, 6, 19, 7, 18, 14, 3]\n",
      "kernel_size: [1, 5, 2, 1, 2, 5, 1]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 2, 2, 3]\n",
      "p: [0.7410706604049652, 0.06651191148108981, 0.18348343923806554, 0.44806689167080804, 0.6202587165871484, 0.08281845101194396, 0.7444348554207728, 0.5535901657849979, 0.4543731706592363]\n",
      "lambdda: 0\n",
      "lr: 0.0656530612244898\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [44, 33, 2]\n",
      "layers_conv: [28, 9, 2]\n",
      "kernel_size: [5, 7]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.6921925810106018, 0.9557523703866001, 0.7920928664033252, 0.5534938565513079]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.06969387755102041\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 6\n",
      "layers_fc: [5, 2]\n",
      "layers_conv: [28, 15, 16, 17, 1, 9, 2]\n",
      "kernel_size: [3, 5, 6, 4, 3, 6]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 2, 2]\n",
      "p: [0.47860226706931663, 0.839301172715151, 0.6372564129055295, 0.9259765614913429, 0.05646047592891923, 0.23099464590765129, 0.8000049019965825]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.031306122448979595\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [40, 26, 12, 2]\n",
      "layers_conv: [28, 10, 15, 6, 4]\n",
      "kernel_size: [4, 2, 7, 7]\n",
      "    pooling_kernel_size: [3, 3, 3, 2]\n",
      "p: [0.3049179744708087, 0.4585652082410434, 0.9414341380882275, 0.16767278643627026, 0.30597191032479343, 0.2730514719045879, 0.035541753033796475]\n",
      "lambdda: 0.04724137931034483\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 5\n",
      "layers_fc: [2, 5, 2]\n",
      "layers_conv: [28, 1, 9, 8, 2, 12]\n",
      "kernel_size: [2, 7, 7, 3, 4]\n",
      "    pooling_kernel_size: [3, 2, 3, 2, 3]\n",
      "p: [0.24983387066526686, 0.9452797398936429, 0.30018332636286416, 0.4603523467380657, 0.6960682666211423, 0.4184630002666574, 0.4467590698067717]\n",
      "lambdda: 0.05655172413793104\n",
      "lr: 0.07979591836734694\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 7\n",
      "layers_fc: [19, 2]\n",
      "layers_conv: [28, 12, 16, 11, 2, 8, 2, 1]\n",
      "kernel_size: [6, 4, 5, 4, 5, 5, 5]\n",
      "    pooling_kernel_size: [3, 2, 3, 2, 3, 3, 2]\n",
      "p: [0.029215480604674493, 0.07727369693872599, 0.21453217922522216, 0.4952501123433921, 0.8947173547264725, 0.32363701201890793, 0.8673563782902467, 0.28259356649397416]\n",
      "lambdda: 0.03793103448275863\n",
      "lr: 0.009081632653061226\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [8, 28, 2]\n",
      "layers_conv: [28, 16, 1, 4, 2, 6, 6, 10]\n",
      "kernel_size: [5, 5, 2, 7, 1, 1, 4]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 2, 2, 2]\n",
      "p: [0.10801571942433652, 0.6017255004135127, 0.7429409519811773, 0.7660095259447055, 0.9283236777650112, 0.5081091659016954, 0.8077849203087141, 0.5809995642315195, 0.7550516238398581]\n",
      "lambdda: 0.019310344827586208\n",
      "lr: 0.04948979591836735\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [16, 10, 2]\n",
      "layers_conv: [28, 19, 2, 16, 7, 14, 8]\n",
      "kernel_size: [3, 4, 4, 5, 2, 1]\n",
      "    pooling_kernel_size: [2, 2, 3, 2, 3, 3]\n",
      "p: [0.3923033558396072, 0.09554248373767449, 0.871338574691061, 0.7502343847578664, 0.5797894196929139, 0.21897478330643916, 0.19827899581665076, 0.7321574887663147]\n",
      "lambdda: 0.02241379310344828\n",
      "lr: 0.07373469387755102\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [5, 27, 27, 2]\n",
      "layers_conv: [28, 11, 12, 18, 9, 7]\n",
      "kernel_size: [3, 7, 4, 7, 7]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 2]\n",
      "p: [0.02421877835427111, 0.24260661265658923, 0.9678695370012012, 0.1383043766975811, 0.5503151698383576, 0.005451070875182151, 0.3064799676712131, 0.44471926214717117]\n",
      "lambdda: 0.07827586206896552\n",
      "lr: 0.07979591836734694\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 7\n",
      "layers_fc: [28, 34, 2]\n",
      "layers_conv: [28, 4, 3, 8, 2, 3, 17, 19]\n",
      "kernel_size: [3, 3, 2, 7, 5, 7, 5]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 2, 3, 2]\n",
      "p: [0.6297869002964175, 0.9247553768372959, 0.816295612858086, 0.5972776395637491, 0.6201616126695377, 0.49905467215096533, 0.15544063416477127, 0.9352916981340116, 0.5788685962539953]\n",
      "lambdda: 0.07827586206896552\n",
      "lr: 0.02726530612244898\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 5\n",
      "layers_fc: [24, 2]\n",
      "layers_conv: [28, 10, 12, 10, 11, 18]\n",
      "kernel_size: [2, 6, 4, 7, 3]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 3]\n",
      "p: [0.3768030542769124, 0.7900985124725356, 0.0992864523671606, 0.702193798867497, 0.6698715920520029, 0.6723716962055408]\n",
      "lambdda: 0.04724137931034483\n",
      "lr: 0.08585714285714287\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 1\n",
      "layers_fc: [18, 33, 2]\n",
      "layers_conv: [28, 6]\n",
      "kernel_size: [3]\n",
      "    pooling_kernel_size: [3]\n",
      "p: [0.46376420093595994, 0.17636858060546967, 0.39389235285855007]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.05353061224489796\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [5, 26, 2]\n",
      "layers_conv: [28, 18, 4, 9, 17, 7, 8]\n",
      "kernel_size: [7, 2, 7, 3, 5, 2]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 3, 2]\n",
      "p: [0.6462667979768331, 0.6843422314540248, 0.07469616379606148, 0.33457209072886873, 0.9826724664044821, 0.01879474131521408, 0.34885501541836195, 0.061545247231411415]\n",
      "lambdda: 0.01310344827586207\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [15, 17, 24, 2]\n",
      "layers_conv: [28, 14, 11, 10, 4]\n",
      "kernel_size: [5, 4, 5, 1]\n",
      "    pooling_kernel_size: [2, 2, 3, 3]\n",
      "p: [0.44464313551152346, 0.24556961136659117, 0.30517259278445075, 0.7276273638430578, 0.15241135395313188, 0.6321471578696279, 0.23839046244274575]\n",
      "lambdda: 0.034827586206896556\n",
      "lr: 0.051510204081632656\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [18, 39, 2]\n",
      "layers_conv: [28, 1, 5]\n",
      "kernel_size: [1, 4]\n",
      "    pooling_kernel_size: [2, 3]\n",
      "p: [0.8214738417316084, 0.49268920018878204, 0.40985807245234007, 0.514576766682755]\n",
      "lambdda: 0.07206896551724139\n",
      "lr: 0.0979795918367347\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 8\n",
      "layers_fc: [4, 25, 11, 2]\n",
      "layers_conv: [28, 14, 14, 15, 4, 13, 9, 1, 17]\n",
      "kernel_size: [4, 1, 7, 3, 6, 4, 7, 1]\n",
      "    pooling_kernel_size: [2, 2, 2, 3, 2, 3, 2, 2]\n",
      "p: [0.2219236682086515, 0.5362737347926029, 0.9625983581850133, 0.17280599161527077, 0.6272988593737363, 0.9177814967801973, 0.3249264623792676, 0.6912716198392174, 0.7946758558490854, 0.970971288063572, 0.8371980932751579]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.051510204081632656\n",
      "num_layers_fc: 1\n",
      "num_layers_conv: 4\n",
      "layers_fc: [6, 2]\n",
      "layers_conv: [28, 7, 10, 18, 7]\n",
      "kernel_size: [5, 6, 1, 6]\n",
      "    pooling_kernel_size: [2, 3, 3, 3]\n",
      "p: [0.8234019896120373, 0.24195307079404949, 0.3359970221356645, 0.3290075222824772, 0.6057538697379844]\n",
      "lambdda: 0.04103448275862069\n",
      "lr: 0.019183673469387756\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [41, 27, 9, 2]\n",
      "layers_conv: [28, 3, 12, 18, 7, 7]\n",
      "kernel_size: [7, 4, 6, 7, 3]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2]\n",
      "p: [0.8410383391325764, 0.5806577895859394, 0.827954416410003, 0.7132844138532857, 0.9191040714485899, 0.041002313079056885, 0.23955777546907275, 0.2147953307271675]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.0959591836734694\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 4\n",
      "layers_fc: [9, 41, 24, 2]\n",
      "layers_conv: [28, 13, 14, 14, 8]\n",
      "kernel_size: [2, 1, 4, 7]\n",
      "    pooling_kernel_size: [2, 2, 3, 2]\n",
      "p: [0.22041742440352818, 0.08564977878769187, 0.1931203808184544, 0.008870761502791202, 0.6105581355963013, 0.36894411470209065, 0.6888546617389031]\n",
      "lambdda: 0.09689655172413793\n",
      "lr: 0.047469387755102045\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 2\n",
      "layers_fc: [48, 16, 2]\n",
      "layers_conv: [28, 1, 18]\n",
      "kernel_size: [5, 2]\n",
      "    pooling_kernel_size: [3, 3]\n",
      "p: [0.43992851215535955, 0.3346074924669692, 0.17738157386640696, 0.5429326862125774]\n",
      "lambdda: 0.028620689655172414\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [29, 30, 37, 2]\n",
      "layers_conv: [28, 10, 14, 14, 2, 19, 7]\n",
      "kernel_size: [5, 7, 7, 4, 6, 7]\n",
      "    pooling_kernel_size: [2, 2, 2, 2, 2, 3]\n",
      "p: [0.510997769161112, 0.20937780707994802, 0.10012275114757696, 0.9427109818739805, 0.7095140431383601, 0.7805158421329189, 0.5901757310127579, 0.17983263465186083, 0.6123718407841086]\n",
      "lambdda: 0.05344827586206897\n",
      "lr: 0.09393877551020409\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 6\n",
      "layers_fc: [41, 3, 2]\n",
      "layers_conv: [28, 6, 15, 14, 3, 9, 16]\n",
      "kernel_size: [6, 4, 1, 3, 4, 5]\n",
      "    pooling_kernel_size: [2, 2, 2, 3, 2, 3]\n",
      "p: [0.9773576006491508, 0.2321489241907876, 0.5284211327950464, 0.3251345608835343, 0.29697269114790925, 0.8849748014320953, 0.5076737986878055, 0.07059676446568008]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.08787755102040817\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [11, 8, 28, 2]\n",
      "layers_conv: [28, 8, 11, 17, 11, 3]\n",
      "kernel_size: [5, 7, 2, 1, 6]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 2]\n",
      "p: [0.044079920273608075, 0.9281788120234399, 0.13903644038099738, 0.9179907707630368, 0.2317668239305143, 0.7088732656924457, 0.7415257385382585, 0.17243388644313484]\n",
      "lambdda: 0.0813793103448276\n",
      "lr: 0.09191836734693878\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [24, 28, 49, 2]\n",
      "layers_conv: [28, 5, 1, 13, 11, 2, 14, 3, 8, 4]\n",
      "kernel_size: [5, 2, 7, 7, 1, 5, 5, 7, 7]\n",
      "    pooling_kernel_size: [3, 3, 3, 2, 2, 2, 3, 2, 3]\n",
      "p: [0.15706592943062325, 0.46978868234777593, 0.19140692848257757, 0.9972239128526313, 0.9776365932135239, 0.589508570364628, 0.5965334896309534, 0.710911809109053, 0.1029990767999982, 0.22339470244049142, 0.7526241668531115, 0.4006154720497481]\n",
      "lambdda: 0.07517241379310345\n",
      "lr: 0.0959591836734694\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 6\n",
      "layers_fc: [27, 22, 22, 2]\n",
      "layers_conv: [28, 14, 8, 7, 3, 1, 6]\n",
      "kernel_size: [2, 3, 7, 1, 1, 6]\n",
      "    pooling_kernel_size: [3, 3, 3, 3, 3, 2]\n",
      "p: [0.6398229390259202, 0.11350356723260768, 0.15932208333946962, 0.7622477156381877, 0.08042462043405452, 0.08972269288481194, 0.7236099761110761, 0.6815077474379961, 0.7102727994874345]\n",
      "lambdda: 0.06896551724137931\n",
      "lr: 0.0676734693877551\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 3\n",
      "layers_fc: [35, 45, 2]\n",
      "layers_conv: [28, 5, 15, 4]\n",
      "kernel_size: [1, 5, 2]\n",
      "    pooling_kernel_size: [3, 3, 3]\n",
      "p: [0.5678147083145875, 0.6277027887436083, 0.09012463242771529, 0.8962750152810199, 0.9295256556684353]\n",
      "lambdda: 0.08448275862068966\n",
      "lr: 0.025244897959183676\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 5\n",
      "layers_fc: [40, 23, 2, 2]\n",
      "layers_conv: [28, 14, 16, 1, 17, 10]\n",
      "kernel_size: [6, 4, 1, 6, 3]\n",
      "    pooling_kernel_size: [2, 2, 3, 3, 3]\n",
      "p: [0.06465032873579024, 0.13208350564448756, 0.6881641072140258, 0.28103904302683713, 0.027399572720599785, 0.9872371587931444, 0.4390126319376866, 0.42218933670605585]\n",
      "lambdda: 0.06586206896551725\n",
      "lr: 0.05757142857142858\n",
      "num_layers_fc: 3\n",
      "num_layers_conv: 9\n",
      "layers_fc: [26, 30, 20, 2]\n",
      "layers_conv: [28, 11, 10, 3, 2, 11, 2, 1, 3, 16]\n",
      "kernel_size: [5, 2, 7, 5, 1, 7, 2, 4, 7]\n",
      "    pooling_kernel_size: [3, 3, 2, 3, 3, 3, 3, 3, 2]\n",
      "p: [0.7640661994306939, 0.1598632267892267, 0.4536728198412209, 0.769256492626789, 0.13790344881750716, 0.654163949131933, 0.9553455462884003, 0.44209447886318864, 0.5331285640586747, 0.36307268976183626, 0.45514862345453344, 0.26766637789152237]\n",
      "lambdda: 0.050344827586206904\n",
      "lr: 0.009081632653061226\n",
      "num_layers_fc: 2\n",
      "num_layers_conv: 8\n",
      "layers_fc: [4, 48, 2]\n",
      "layers_conv: [28, 16, 5, 5, 4, 12, 10, 7, 10]\n",
      "kernel_size: [1, 3, 5, 4, 1, 5, 1, 4]\n",
      "    pooling_kernel_size: [2, 3, 2, 2, 3, 3, 2, 2]\n",
      "p: [0.23977062307782637, 0.2441065854149409, 0.7462942724679091, 0.7626263364459573, 0.5285179740283793, 0.6847252602078971, 0.26859059422644105, 0.6888329232366858, 0.39500562464397637, 0.31382986408891167]\n",
      "lambdda: 0.016206896551724137\n",
      "lr: 0.04544897959183674\n"
     ]
    }
   ],
   "source": [
    "run_list = []\n",
    "for i in range (100):\n",
    "    try:\n",
    "        # param_num_layers_fc\n",
    "        num_layers_fc = np.random.randint(1, 4)\n",
    "        #param_num_layers_conv\n",
    "        num_layers_conv = np.random.randint(1, 10)\n",
    "        layers_fc = np.random.randint(1, 50, num_layers_fc).tolist()+[2]\n",
    "        layers_conv = [28] + np.random.randint(1,20, num_layers_conv).tolist()\n",
    "        kernel_size = np.random.randint(1, 8, num_layers_conv).tolist()\n",
    "        pooling_kernel_size = np.random.randint(2, 4, num_layers_conv).tolist()\n",
    "        p = np.random.rand(num_layers_fc+num_layers_conv).tolist()\n",
    "        lambdda = parameters['lambda'][np.random.randint(1, len(parameters['lambda']))]\n",
    "        lr = parameters['lr'][np.random.randint(1, len(parameters['lr']))]\n",
    "        print(\"num_layers_fc: {}\\nnum_layers_conv: {}\\nlayers_fc: {}\\nlayers_conv: {}\\nkernel_size: {}\\n\\\n",
    "    pooling_kernel_size: {}\\np: {}\\nlambdda: {}\\nlr: {}\"\\\n",
    "              .format(num_layers_fc, num_layers_conv, layers_fc, layers_conv, kernel_size, \\\n",
    "                      pooling_kernel_size,p, lambdda, lr))\n",
    "        param = {'type': 'conv', 'layers': layers_fc, 'layers_conv':layers_conv, 'kernel_size': kernel_size, \\\n",
    "                  'pooling_kernel_size': pooling_kernel_size, 'p_list': p}\n",
    "        run_params = {'param': param, 'train_input': train_input, 'train_target': train_target, 'kfolds': kfolds, 'nb_epochs': nb_epochs, 'lambdda': lambdda, 'lr': lr}\n",
    "        run_list.append(run_params)\n",
    "        #loss_train_kfold, loss_val_kfold, acc_train_kfold, acc_val_kfold = train_model_full(param, train_input, train_target, kfolds, nb_epochs, lambdda, lr)\n",
    "    except: pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e33584a55a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-25:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n",
      "Process ForkPoolWorker-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 344, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'pr' on <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "pool.map(train_parallel, run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add feature with time\n",
    "#steap and flat for right/left\n",
    "#throw away some features, i.e. feature 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test import pr\n",
    "pool = Pool(4)\n",
    "pool.map(pr, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "2\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.504 \n",
      "   Loss: val ~ 0.696 / Acc val ~ 0.51\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.694 / Acc val ~ 0.5\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.505 \n",
      "   Loss: val ~ 0.696 / Acc val ~ 0.471\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.507 \n",
      "   Loss: val ~ 0.695 / Acc val ~ 0.484\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.498 \n",
      "   Loss: val ~ 0.693 / Acc val ~ 0.481\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.508 \n",
      "   Loss: val ~ 0.694 / Acc val ~ 0.465\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.695 / Acc val ~ 0.504\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.5 \n",
      "   Loss: val ~ 0.701 / Acc val ~ 0.471\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.694 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.695 / Acc val ~ 0.488\n",
      "\n",
      "\n",
      "\n",
      "---- Epochs Done -----\n",
      "\n",
      "Loss: train ~ 0.693 Acc train ~ 0.503 \n",
      "   Loss: val ~ 0.694 / Acc val ~ 0.5\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensor.c:24",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/ana/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/Users/ana/Desktop/deeplearning/proj1-dl/src/proj1-dl/Project.py\", line 366, in train_parallel\n    train_model_full(**param)\n  File \"/Users/ana/Desktop/deeplearning/proj1-dl/src/proj1-dl/Project.py\", line 86, in train_model_full\n    elif param['type']=='conv': model = model_conv = Conv_net(param['layers'], param['layers_conv'],                                                     param['kernel_size'], param['pooling_kernel_size'], param['p_list'])\n  File \"/Users/ana/Desktop/deeplearning/proj1-dl/src/proj1-dl/Project.py\", line 285, in __init__\n    self.additional_fc_hidden.append(nn.Linear(size*layers_conv[-1], layers[0]))\n  File \"/Users/ana/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 46, in __init__\n    self.reset_parameters()\n  File \"/Users/ana/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 49, in reset_parameters\n    stdv = 1. / math.sqrt(self.weight.size(1))\nRuntimeError: invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensor.c:24\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-79934fe54f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensor.c:24"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from Project import train_parallel, run_list\n",
    "pool = Pool(4)\n",
    "pool.map(pr, [1,2,3])\n",
    "\n",
    "pool.map(train_parallel, run_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
